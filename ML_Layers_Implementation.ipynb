{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzu3CqupNw0-"
      },
      "source": [
        "# Реализация основных слоев для машинного обучения\n",
        "\n",
        "В этом ноутбуке вы изучите и реализуете основные компоненты нейронных сетей:\n",
        "- Функции активации (ReLU, Sigmoid, Tanh)\n",
        "- Линейный слой (Linear)\n",
        "- Последовательный контейнер (Sequential)\n",
        "- Регуляризация (Dropout)\n",
        "- Нормализация (BatchNorm)\n",
        "\n",
        "Каждый блок содержит теоретическое объяснение, шаблон для реализации и тесты для проверки.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "W2Pan53_Nw0_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, List\n",
        "\n",
        "# Установим seed для воспроизводимости\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEcWgFe_Nw0_"
      },
      "source": [
        "## Базовый класс для всех слоев\n",
        "\n",
        "Сначала определим базовый класс, от которого будут наследоваться все наши слои:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GtSXjzbVNw1A"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    Базовый класс для всех слоев нейронной сети\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямое распространение\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Обратное распространение\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Переключение в режим обучения\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Переключение в режим инференса\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrJLtjqxNw1A"
      },
      "source": [
        "## 1. Функция активации ReLU\n",
        "\n",
        "### Теория\n",
        "\n",
        "**ReLU (Rectified Linear Unit)** - одна из самых популярных функций активации в современных нейронных сетях.\n",
        "\n",
        "**Формула:**\n",
        "- Forward: `f(x) = max(0, x)`\n",
        "- Backward: `df/dx = 1 если x > 0, иначе 0`\n",
        "\n",
        "**Преимущества:**\n",
        "- Простота вычислений\n",
        "- Решает проблему затухающих градиентов\n",
        "- Разреженность активаций\n",
        "\n",
        "**Недостатки:**\n",
        "- \"Мертвые нейроны\" (dying ReLU problem)\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "L5BJaEr1Nw1A"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямое распространение для ReLU\n",
        "\n",
        "        Args:\n",
        "            x: входной тензор формы (batch_size, ...)\n",
        "\n",
        "        Returns:\n",
        "            выходной тензор той же формы\n",
        "        \"\"\"\n",
        "        # TODO: Сохраните входные данные для backward pass\n",
        "        self.input = x\n",
        "\n",
        "        # TODO: Реализуйте ReLU функцию\n",
        "        output = np.maximum(0, x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Обратное распространение для ReLU\n",
        "\n",
        "        Args:\n",
        "            grad_output: градиент от следующего слоя\n",
        "\n",
        "        Returns:\n",
        "            градиент для предыдущего слоя\n",
        "        \"\"\"\n",
        "        # TODO: Реализуйте градиент ReLU\n",
        "        grad_input = grad_output.copy()\n",
        "        grad_input[self.input <= 0] = 0\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsVxEC23Nw1A"
      },
      "source": [
        "### Тестирование ReLU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C3rHRLDNw1A",
        "outputId": "946ae6e6-b9c8-4f14-c671-fd9c000037c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [[-2. -1.  0.  1.  2.]]\n",
            "Output: [[0. 0. 0. 1. 2.]]\n",
            "Expected: [[0. 0. 0. 1. 2.]]\n",
            "Gradient output: [[1. 1. 1. 1. 1.]]\n",
            "Gradient input: [[0. 0. 0. 1. 1.]]\n",
            "Expected gradient: [[0. 0. 0. 1. 1.]]\n",
            "✅ ReLU тест пройден успешно!\n"
          ]
        }
      ],
      "source": [
        "# Тест ReLU (запустите этот код после реализации ReLU)\n",
        "relu = ReLU()\n",
        "\n",
        "# Тестовые данные\n",
        "x_test = np.array([[-2, -1, 0, 1, 2]], dtype=np.float32)\n",
        "expected_forward = np.array([[0, 0, 0, 1, 2]], dtype=np.float32)\n",
        "\n",
        "# Forward pass\n",
        "output = relu.forward(x_test)\n",
        "print(f\"Input: {x_test}\")\n",
        "print(f\"Output: {output}\")\n",
        "print(f\"Expected: {expected_forward}\")\n",
        "\n",
        "# Проверка forward pass\n",
        "assert np.allclose(output, expected_forward), \"ReLU forward pass не работает корректно!\"\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.ones_like(output)\n",
        "grad_input = relu.backward(grad_output)\n",
        "expected_backward = np.array([[0, 0, 0, 1, 1]], dtype=np.float32)\n",
        "\n",
        "print(f\"Gradient output: {grad_output}\")\n",
        "print(f\"Gradient input: {grad_input}\")\n",
        "print(f\"Expected gradient: {expected_backward}\")\n",
        "\n",
        "# Проверка backward pass\n",
        "assert np.allclose(grad_input, expected_backward), \"ReLU backward pass не работает корректно!\"\n",
        "\n",
        "print(\"✅ ReLU тест пройден успешно!\")\n",
        "\n",
        "# print(\"⚠️ Реализуйте ReLU класс выше, затем раскомментируйте этот код для тестирования\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQOeatDkNw1B"
      },
      "source": [
        "## 2. Функция активации Sigmoid\n",
        "\n",
        "### Теория\n",
        "\n",
        "**Sigmoid** - классическая функция активации, которая \"сжимает\" входные значения в диапазон (0, 1).\n",
        "\n",
        "**Формула:**\n",
        "- Forward: `f(x) = 1 / (1 + exp(-x))`\n",
        "- Backward: `df/dx = f(x) * (1 - f(x))`\n",
        "\n",
        "**Применение:**\n",
        "- Бинарная классификация (выходной слой)\n",
        "- Gating механизмы (LSTM, GRU)\n",
        "\n",
        "**Проблемы:**\n",
        "- Затухающие градиенты при глубоких сетях\n",
        "- Насыщение при больших значениях\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "yRcim4xwNw1B"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямое распространение для Sigmoid\n",
        "\n",
        "        Args:\n",
        "            x: входной тензор\n",
        "\n",
        "        Returns:\n",
        "            выходной тензор той же формы, значения в диапазоне (0, 1)\n",
        "        \"\"\"\n",
        "        # TODO: Реализуйте sigmoid функцию\n",
        "        self.output = 1/(1+ np.exp(-x))\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Обратное распространение для Sigmoid\n",
        "\n",
        "        Args:\n",
        "            grad_output: градиент от следующего слоя\n",
        "\n",
        "        Returns:\n",
        "            градиент для предыдущего слоя\n",
        "        \"\"\"\n",
        "        # TODO: Реализуйте градиент sigmoid\n",
        "        sigmoid_derivative = self.output * (1 - self.output)\n",
        "        grad_input = sigmoid_derivative * grad_output\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLIXbIyfNw1B"
      },
      "source": [
        "### Тестирование Sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8vhcatuNw1B",
        "outputId": "60b634af-29fd-4194-8162-dbb379c2fec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [[-10.  -1.   0.   1.  10.]]\n",
            "Output: [[4.539787e-05 2.689414e-01 5.000000e-01 7.310586e-01 9.999546e-01]]\n",
            "Gradient input: [[0.19661193 0.19661193 0.19661193 0.19661193 0.19661193]]\n",
            "✅ Sigmoid тест пройден успешно!\n"
          ]
        }
      ],
      "source": [
        "# Тест Sigmoid (запустите этот код после реализации Sigmoid)\n",
        "# print(\"⚠️ Реализуйте Sigmoid класс выше, затем раскомментируйте этот код для тестирования\")\n",
        "\n",
        "sigmoid = Sigmoid()\n",
        "\n",
        "# Тестовые данные\n",
        "x_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.float32)\n",
        "\n",
        "# Forward pass\n",
        "output = sigmoid.forward(x_test)\n",
        "print(f\"Input: {x_test}\")\n",
        "print(f\"Output: {output}\")\n",
        "\n",
        "# Проверим, что выходные значения в диапазоне (0, 1)\n",
        "assert np.all(output > 0) and np.all(output < 1), \"Sigmoid должен возвращать значения в диапазоне (0, 1)\"\n",
        "\n",
        "# Проверим симметричность: sigmoid(-x) = 1 - sigmoid(x)\n",
        "x_sym = np.array([[1]], dtype=np.float32)\n",
        "out_pos = sigmoid.forward(x_sym)\n",
        "out_neg = sigmoid.forward(-x_sym)\n",
        "assert np.allclose(out_neg, 1 - out_pos, atol=1e-6), \"Sigmoid должен быть симметричным\"\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.ones_like(output)\n",
        "grad_input = sigmoid.backward(grad_output)\n",
        "print(f\"Gradient input: {grad_input}\")\n",
        "\n",
        "# Проверим, что градиент положительный (sigmoid монотонно возрастает)\n",
        "assert np.all(grad_input >= 0), \"Градиент Sigmoid должен быть неотрицательным\"\n",
        "\n",
        "print(\"✅ Sigmoid тест пройден успешно!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDBOD5ztNw1B"
      },
      "source": [
        "## 3. Функция активации Tanh\n",
        "\n",
        "### Теория\n",
        "\n",
        "**Tanh (гиперболический тангенс)** - функция активации, которая \"сжимает\" входные значения в диапазон (-1, 1).\n",
        "\n",
        "**Формула:**\n",
        "- Forward: `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n",
        "- Backward: `df/dx = 1 - f(x)²`\n",
        "\n",
        "**Преимущества над Sigmoid:**\n",
        "- Выход центрирован вокруг нуля\n",
        "- Больший диапазон градиентов\n",
        "- Связь с sigmoid: `tanh(x) = 2*sigmoid(2x) - 1`\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9ExyvVq8Nw1B"
      },
      "outputs": [],
      "source": [
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямое распространение для Tanh\n",
        "\n",
        "        Args:\n",
        "            x: входной тензор\n",
        "\n",
        "        Returns:\n",
        "            выходной тензор той же формы, значения в диапазоне (-1, 1)\n",
        "        \"\"\"\n",
        "        # TODO: Реализуйте tanh функцию\n",
        "        self.output = np.tanh(x)\n",
        "        self.output = np.clip(self.output, -0.999999, 0.999999)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Обратное распространение для Tanh\n",
        "\n",
        "        Args:\n",
        "            grad_output: градиент от следующего слоя\n",
        "\n",
        "        Returns:\n",
        "            градиент для предыдущего слоя\n",
        "        \"\"\"\n",
        "        # TODO: Реализуйте градиент tanh\n",
        "\n",
        "        tanh_derivative = 1 - self.output**2\n",
        "        grad_input = tanh_derivative * grad_output\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKX-bVwyNw1C"
      },
      "source": [
        "### Тестирование Tanh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujjVvkwHNw1C",
        "outputId": "8aa97958-e49a-4d16-b313-95026159a504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [[-10.  -1.   0.   1.  10.]]\n",
            "Output: [[-0.999999  -0.7615942  0.         0.7615942  0.999999 ]]\n",
            "Gradient input: [[1. 1. 1. 1. 1.]]\n",
            "✅ Tanh тест пройден успешно!\n"
          ]
        }
      ],
      "source": [
        "# Тест Tanh (запустите этот код после реализации Tanh)\n",
        "# print(\"⚠️ Реализуйте Tanh класс выше, затем раскомментируйте этот код для тестирования\")\n",
        "\n",
        "tanh = Tanh()\n",
        "\n",
        "# Тестовые данные\n",
        "x_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.float32)\n",
        "\n",
        "# Forward pass\n",
        "output = tanh.forward(x_test)\n",
        "print(f\"Input: {x_test}\")\n",
        "print(f\"Output: {output}\")\n",
        "\n",
        "# Проверим, что выходные значения в диапазоне (-1, 1)\n",
        "assert np.all(output > -1) and np.all(output < 1), \"Tanh должен возвращать значения в диапазоне (-1, 1)\"\n",
        "\n",
        "# Проверим антисимметричность: tanh(-x) = -tanh(x)\n",
        "x_antisym = np.array([[2]], dtype=np.float32)\n",
        "out_pos = tanh.forward(x_antisym)\n",
        "out_neg = tanh.forward(-x_antisym)\n",
        "assert np.allclose(out_neg, -out_pos, atol=1e-6), \"Tanh должен быть антисимметричным\"\n",
        "\n",
        "# Проверим, что tanh(0) = 0\n",
        "zero_out = tanh.forward(np.array([[0]], dtype=np.float32))\n",
        "assert np.allclose(zero_out, 0, atol=1e-6), \"tanh(0) должен быть равен 0\"\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.ones_like(output)\n",
        "grad_input = tanh.backward(grad_output)\n",
        "print(f\"Gradient input: {grad_input}\")\n",
        "\n",
        "# Проверим, что градиент положительный (tanh монотонно возрастает)\n",
        "assert np.all(grad_input >= 0), \"Градиент Tanh должен быть неотрицательным\"\n",
        "\n",
        "print(\"✅ Tanh тест пройден успешно!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggxGupOQNw1C"
      },
      "source": [
        "## 4. Линейный слой (Linear/Dense)\n",
        "\n",
        "### Теория\n",
        "\n",
        "**Линейный слой** - основной строительный блок нейронных сетей, выполняющий аффинное преобразование.\n",
        "\n",
        "**Формула:**\n",
        "- Forward: `y = x @ W + b`\n",
        "- где W - матрица весов размера (input_size, output_size)\n",
        "- b - вектор смещений размера (output_size,)\n",
        "\n",
        "**Градиенты:**\n",
        "- `∂L/∂x = grad_output @ W.T`\n",
        "- `∂L/∂W = x.T @ grad_output`\n",
        "- `∂L/∂b = sum(grad_output, axis=0)`\n",
        "\n",
        "**Инициализация весов:**\n",
        "- Xavier/Glorot: помогает поддерживать дисперсию активаций\n",
        "- He: оптимизирована для ReLU активаций\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rnNouHNtNw1C"
      },
      "outputs": [],
      "source": [
        "class Linear(Layer):\n",
        "    def __init__(self, input_size, output_size, bias=True):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.use_bias = bias\n",
        "\n",
        "        # TODO: Инициализируйте веса\n",
        "        std = np.sqrt(2.0 / input_size)\n",
        "        self.weight = np.random.randn(input_size,output_size) * std #method kaiming\n",
        "\n",
        "        # TODO: Инициализируйте bias (если используется)\n",
        "        if self.use_bias:\n",
        "            self.bias = np.zeros(output_size)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # Переменные для сохранения входных данных и градиентов\n",
        "        self.input = None\n",
        "        self.grad_weight = np.zeros_like(self.weight)\n",
        "        if self.use_bias:\n",
        "            self.grad_bias = np.zeros_like(self.bias)\n",
        "        else:\n",
        "            self.grad_bias = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямое распространение для линейного слоя\n",
        "\n",
        "        Args:\n",
        "            x: входной тензор формы (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            выходной тензор формы (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        # TODO: Сохраните входные данные для backward pass\n",
        "        self.input = x.copy()\n",
        "\n",
        "        # TODO: Реализуйте линейное преобразование\n",
        "        output = x @ self.weight\n",
        "\n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Обратное распространение для линейного слоя\n",
        "\n",
        "        Args:\n",
        "            grad_output: градиент от следующего слоя формы (batch_size, output_size)\n",
        "\n",
        "        Returns:\n",
        "            градиент для предыдущего слоя формы (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        # TODO: Вычислите градиент по входу\n",
        "        grad_input = grad_output @ self.weight.T\n",
        "\n",
        "        # TODO: Вычислите градиент по весам\n",
        "        self.grad_weight += self.input.T @ grad_output\n",
        "\n",
        "        # TODO: Вычислите градиент по bias\n",
        "        if self.use_bias:\n",
        "            self.grad_bias += np.sum(grad_output, axis=0)\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "    def update_weights(self, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Обновление весов с помощью градиентного спуска\n",
        "        \"\"\"\n",
        "        if self.grad_weight is not None:\n",
        "            self.weight -= learning_rate * self.grad_weight\n",
        "\n",
        "        if self.use_bias and self.grad_bias is not None:\n",
        "            self.bias -= learning_rate * self.grad_bias\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Обнуление градиентов\n",
        "        \"\"\"\n",
        "        self.grad_weight = np.zeros_like(self.grad_weight)\n",
        "        if self.use_bias:\n",
        "            self.grad_bias = np.zeros_like(self.grad_bias)\n",
        "\n",
        "    def parameters(self):\n",
        "      if self.bias is not None:\n",
        "        return (self.weight, self.bias)\n",
        "      else:\n",
        "        return(self.weight,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIokQDGPNw1C"
      },
      "source": [
        "### Тестирование Linear\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2n-G_3qPNw1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fbff26-4b4b-43f5-a52b-7dc7fe1d8984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Веса: \n",
            "[[ 0.40556541 -0.11289233]\n",
            " [ 0.52883548  1.24354867]\n",
            " [-0.19118543 -0.19117202]]\n",
            "Bias: [0. 0.]\n",
            "Input shape: (4, 3)\n",
            "Output shape: (4, 2)\n",
            "Expected shape: (4, 2)\n",
            "Gradient input shape: (4, 3)\n",
            "Gradient weight shape: (3, 2)\n",
            "Gradient bias shape: (2,)\n",
            "✅ Linear тест пройден успешно!\n"
          ]
        }
      ],
      "source": [
        "# Тест Linear (запустите этот код после реализации Linear)\n",
        "# print(\"⚠️ Реализуйте Linear класс выше, затем раскомментируйте этот код для тестирования\")\n",
        "\n",
        "linear = Linear(input_size=3, output_size=2, bias=True)\n",
        "\n",
        "# Проверим форму весов\n",
        "assert linear.weight.shape == (3, 2), f\"Неправильная форма весов: {linear.weight.shape}\"\n",
        "assert linear.bias.shape == (2,), f\"Неправильная форма bias: {linear.bias.shape}\"\n",
        "\n",
        "print(f\"Веса: \\n{linear.weight}\")\n",
        "print(f\"Bias: {linear.bias}\")\n",
        "\n",
        "# Тестовые данные\n",
        "batch_size = 4\n",
        "x_test = np.random.randn(batch_size, 3).astype(np.float32)\n",
        "\n",
        "# Forward pass\n",
        "output = linear.forward(x_test)\n",
        "expected_shape = (batch_size, 2)\n",
        "\n",
        "print(f\"Input shape: {x_test.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Expected shape: {expected_shape}\")\n",
        "\n",
        "assert output.shape == expected_shape, f\"Неправильная форма выхода: {output.shape}\"\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.random.randn(*output.shape).astype(np.float32)\n",
        "grad_input = linear.backward(grad_output)\n",
        "\n",
        "print(f\"Gradient input shape: {grad_input.shape}\")\n",
        "print(f\"Gradient weight shape: {linear.grad_weight.shape}\")\n",
        "print(f\"Gradient bias shape: {linear.grad_bias.shape}\")\n",
        "\n",
        "# Проверим формы градиентов\n",
        "assert grad_input.shape == x_test.shape, \"Неправильная форма градиента по входу\"\n",
        "assert linear.grad_weight.shape == linear.weight.shape, \"Неправильная форма градиента по весам\"\n",
        "assert linear.grad_bias.shape == linear.bias.shape, \"Неправильная форма градиента по bias\"\n",
        "\n",
        "print(\"✅ Linear тест пройден успешно!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXtyfRLPNw1D"
      },
      "source": [
        "## 5. Последовательный контейнер (Sequential)\n",
        "\n",
        "### Теория\n",
        "\n",
        "**Sequential** - контейнер, который позволяет последовательно применять несколько слоев.\n",
        "\n",
        "**Принцип работы:**\n",
        "- Forward: применяет слои по порядку: `output = layer_n(...layer_2(layer_1(input))...)`\n",
        "- Backward: применяет градиенты в обратном порядке\n",
        "\n",
        "**Применение:**\n",
        "- Создание простых feed-forward сетей\n",
        "- Группировка слоев в блоки\n",
        "- Упрощение архитектуры кода\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Z49Atm2dNw1D"
      },
      "outputs": [],
      "source": [
        "class Sequential(Layer):\n",
        "    def __init__(self, *layers):\n",
        "        super().__init__()\n",
        "        self.layers = list(layers)\n",
        "        self.layer_outputs = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        \"\"\"\n",
        "        Добавление слоя в последовательность\n",
        "        \"\"\"\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямое распространение через все слои\n",
        "\n",
        "        Args:\n",
        "            x: входной тензор\n",
        "\n",
        "        Returns:\n",
        "            выходной тензор после прохождения всех слоев\n",
        "        \"\"\"\n",
        "        # TODO: Очистите список промежуточных выходов\n",
        "        self.layer_outputs = []\n",
        "        self.layer_outputs.append(x)\n",
        "        # TODO: Последовательно примените все слои\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            # TODO: Применить слой и сохранить результат\n",
        "            output = layer.forward(output)\n",
        "            self.layer_outputs.append(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Обратное распространение через все слои в обратном порядке\n",
        "\n",
        "        Args:\n",
        "            grad_output: градиент от следующего слоя\n",
        "\n",
        "        Returns:\n",
        "            градиент для предыдущего слоя\n",
        "        \"\"\"\n",
        "        # TODO: Примените backward для всех слоев в обратном порядке\n",
        "        grad = grad_output\n",
        "        for layer in reversed(self.layers):\n",
        "            # TODO: Примените backward для текущего слоя\n",
        "            grad = layer.backward(grad)\n",
        "\n",
        "        return grad\n",
        "\n",
        "\n",
        "    def update_weights(self, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Обновление весов всех слоев\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'update_weights'):\n",
        "                layer.update_weights(learning_rate)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Обнуление градиентов всех слоев\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'zero_grad'):\n",
        "                layer.zero_grad()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Переключение всех слоев в режим обучения\n",
        "        \"\"\"\n",
        "        super().train()\n",
        "        for layer in self.layers:\n",
        "            layer.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        Переключение всех слоев в режим инференса\n",
        "        \"\"\"\n",
        "        super().eval()\n",
        "        for layer in self.layers:\n",
        "            layer.eval()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.layers)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.layers[idx]\n",
        "\n",
        "    def parameters(self):\n",
        "       for layer in self.layers:\n",
        "        params = layer.parameters()\n",
        "        if isinstance (params, tuple):\n",
        "          yield from params\n",
        "        else:\n",
        "          yield params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtrnnTAjNw1D"
      },
      "source": [
        "## 6. Dropout\n",
        "\n",
        "### Теория\n",
        "\n",
        "**Dropout** - техника регуляризации, которая случайно \"выключает\" некоторые нейроны во время обучения.\n",
        "\n",
        "**Принцип работы:**\n",
        "- **Обучение**: каждый нейрон сохраняется с вероятностью `(1 - dropout_rate)`\n",
        "- **Инференс**: все нейроны активны, но выходы масштабируются\n",
        "\n",
        "**Преимущества:**\n",
        "- Предотвращает переобучение\n",
        "- Улучшает обобщающую способность\n",
        "- Эффект ансамбля моделей\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gLmKPCnMNw1D"
      },
      "outputs": [],
      "source": [
        "class Dropout(Layer):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямое распространение для Dropout\n",
        "\n",
        "        Args:\n",
        "            x: входной тензор\n",
        "\n",
        "        Returns:\n",
        "            выходной тензор с примененным dropout (в режиме обучения)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.training:\n",
        "            # TODO: Создайте бинарную маску для dropout\n",
        "            self.mask = (np.random.rand(*x.shape) > self.dropout_rate).astype(np.float32) / (1.0 - self.dropout_rate)\n",
        "\n",
        "            # TODO: Примените маску и масштабирование\n",
        "            output = x * self.mask\n",
        "        else:\n",
        "            # TODO: В режиме инференса\n",
        "            output = x\n",
        "            self.mask = None\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Обратное распространение для Dropout\n",
        "\n",
        "        Args:\n",
        "            grad_output: градиент от следующего слоя\n",
        "\n",
        "        Returns:\n",
        "            градиент для предыдущего слоя\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            # TODO: Примените ту же маску к градиенту\n",
        "            grad_input = grad_output * self.mask\n",
        "        else:\n",
        "            grad_input = grad_output\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEsMw9rcNw1D"
      },
      "source": [
        "### Тестирование Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "T5sAWmYjNw1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc14d689-9550-444c-8784-39c3e306ca6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Режим обучения:\n",
            "Input mean: 1.000\n",
            "Output mean: 1.008\n",
            "Proportion of zeros: 0.496\n",
            "\n",
            "Режим инференса:\n",
            "Output mean: 1.000\n",
            "Proportion of zeros: 0.000\n",
            "\n",
            "Gradient test:\n",
            "Grad input shape: (100, 10)\n",
            "Grad input mean: 1.018\n",
            "✅ Dropout тест пройден успешно!\n"
          ]
        }
      ],
      "source": [
        "# Тест Dropout (запустите после реализации Dropout)\n",
        "# print(\"⚠️ Реализуйте Dropout класс выше, затем раскомментируйте этот код для тестирования\")\n",
        "\n",
        "dropout = Dropout(dropout_rate=0.5)\n",
        "\n",
        "# Тестовые данные\n",
        "x_test = np.ones((100, 10), dtype=np.float32)\n",
        "\n",
        "# Тест в режиме обучения\n",
        "dropout.train()\n",
        "output_train = dropout.forward(x_test)\n",
        "\n",
        "print(f\"Режим обучения:\")\n",
        "print(f\"Input mean: {x_test.mean():.3f}\")\n",
        "print(f\"Output mean: {output_train.mean():.3f}\")\n",
        "print(f\"Proportion of zeros: {(output_train == 0).mean():.3f}\")\n",
        "\n",
        "# Проверим, что часть нейронов \"выключена\"\n",
        "zeros_ratio = (output_train == 0).mean()\n",
        "expected_zeros = 0.5  # dropout_rate\n",
        "assert abs(zeros_ratio - expected_zeros) < 0.1, f\"Неправильная доля нулевых значений: {zeros_ratio}\"\n",
        "\n",
        "# Проверим масштабирование\n",
        "assert abs(output_train.mean() - x_test.mean()) < 0.1, \"Неправильное масштабирование в режиме обучения\"\n",
        "\n",
        "# Тест в режиме инференса\n",
        "dropout.eval()\n",
        "output_eval = dropout.forward(x_test)\n",
        "\n",
        "print(f\"\\nРежим инференса:\")\n",
        "print(f\"Output mean: {output_eval.mean():.3f}\")\n",
        "print(f\"Proportion of zeros: {(output_eval == 0).mean():.3f}\")\n",
        "\n",
        "# В режиме инференса все значения должны остаться\n",
        "assert np.allclose(output_eval, x_test), \"В режиме инференса выход должен совпадать с входом\"\n",
        "\n",
        "# Тест backward pass\n",
        "dropout.train()\n",
        "output_train = dropout.forward(x_test)\n",
        "grad_output = np.ones_like(output_train)\n",
        "grad_input = dropout.backward(grad_output)\n",
        "\n",
        "print(f\"\\nGradient test:\")\n",
        "print(f\"Grad input shape: {grad_input.shape}\")\n",
        "print(f\"Grad input mean: {grad_input.mean():.3f}\")\n",
        "\n",
        "assert grad_input.shape == x_test.shape, \"Неправильная форма градиента\"\n",
        "\n",
        "print(\"✅ Dropout тест пройден успешно!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n6DHudTNw1D"
      },
      "source": [
        "## 7. Batch Normalization\n",
        "\n",
        "### Теория\n",
        "\n",
        "**Batch Normalization** - техника нормализации, которая стабилизирует обучение глубоких сетей.\n",
        "\n",
        "**Принцип работы:**\n",
        "1. Нормализация: `(x - mean) / sqrt(var + eps)`\n",
        "2. Масштабирование и сдвиг: `gamma * normalized + beta`\n",
        "\n",
        "**Преимущества:**\n",
        "- Ускоряет обучение\n",
        "- Позволяет использовать большие learning rate\n",
        "- Уменьшает зависимость от инициализации\n",
        "- Эффект регуляризации\n",
        "\n",
        "**Различия между режимами:**\n",
        "- **Обучение**: используется статистика текущего batch\n",
        "- **Инференс**: используется накопленная статистика\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XNk3AI6iNw1D"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(Layer):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # TODO: Инициализируйте обучаемые параметры gamma и beta\n",
        "        self.gamma = np.ones(num_features)\n",
        "        self.beta = np.zeros(num_features)\n",
        "\n",
        "        # TODO: Инициализируйте накопленную статистику\n",
        "        self.running_mean = np.zeros(num_features)\n",
        "        self.running_var = np.ones(num_features)\n",
        "\n",
        "        # Переменные для backward pass\n",
        "        self.batch_mean = None\n",
        "        self.batch_var = None\n",
        "        self.normalized = None\n",
        "        self.input = None\n",
        "        self.grad_gamma = np.zeros(num_features)\n",
        "        self.grad_beta = np.zeros(num_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Прямое распространение для Batch Normalization\n",
        "\n",
        "        Args:\n",
        "            x: входной тензор формы (batch_size, num_features)\n",
        "\n",
        "        Returns:\n",
        "            нормализованный выходной тензор той же формы\n",
        "        \"\"\"\n",
        "        self.input = x\n",
        "\n",
        "        if self.training:\n",
        "            # TODO: Вычислите статистику текущего batch\n",
        "            self.batch_mean = np.mean(x, axis = 0 )\n",
        "            self.batch_var = np.var(x, axis = 0)\n",
        "\n",
        "            # TODO: Обновите накопленную статистику\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * self.batch_mean\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * self.batch_var\n",
        "\n",
        "            mean = self.batch_mean\n",
        "            var = self.batch_var\n",
        "        else:\n",
        "            # TODO: Используйте накопленную статистику\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        # TODO: Нормализация\n",
        "        self.normalized = (x - mean) / np.sqrt(var + self.eps)\n",
        "\n",
        "        # TODO: Масштабирование и сдвиг\n",
        "        output = self.gamma * self.normalized + self.beta\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Обратное распространение для Batch Normalization\n",
        "        \"\"\"\n",
        "        n = self.input.shape[0]\n",
        "        # TODO: Вычислите градиенты по параметрам\n",
        "        self.grad_gamma = np.sum(grad_output * self.normalized, axis =0)\n",
        "        self.grad_beta = np.sum(grad_output, axis =0)\n",
        "\n",
        "        grad_normalized = grad_output * self.gamma\n",
        "\n",
        "        # TODO: Вычислите градиент по входу\n",
        "        grad_var = np.sum(grad_normalized * (self.input - self.batch_mean), axis=0) * (-0.5) * (self.batch_var + self.eps)**(-1.5)\n",
        "\n",
        "        grad_mean = np.sum(grad_normalized , axis=0) * (-1) / np.sqrt(self.batch_var + self.eps)\n",
        "        + grad_var * np.mean(-2 * (self.input - self.batch_mean), axis=0)\n",
        "\n",
        "        grad_input = (grad_normalized / np.sqrt(self.batch_var + self.eps)) + grad_var * 2 * (self.input - self.batch_mean) / n + grad_mean /n\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "    def update_weights(self, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Обновление параметров\n",
        "        \"\"\"\n",
        "        if self.grad_gamma is not None:\n",
        "            self.gamma -= learning_rate * self.grad_gamma\n",
        "\n",
        "        if self.grad_beta is not None:\n",
        "            self.beta -= learning_rate * self.grad_beta\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Обнуление градиентов\n",
        "        \"\"\"\n",
        "        self.grad_gamma = np.zeros(self.num_features)\n",
        "        self.grad_beta = np.zeros(self.num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "mtVFZ9waNw1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a2bfc7-d94a-439b-fd1e-3c97ab20dddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gamma: [1. 1. 1. 1.]\n",
            "Beta: [0. 0. 0. 0.]\n",
            "Input: \n",
            "[[1. 2. 3. 4.]\n",
            " [2. 3. 4. 5.]\n",
            " [3. 4. 5. 6.]]\n",
            "Input mean per feature: [2. 3. 4. 5.]\n",
            "Input std per feature: [0.8164966 0.8164966 0.8164966 0.8164966]\n",
            "\n",
            "Output: \n",
            "[[-1.22473562 -1.22473562 -1.22473562 -1.22473562]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 1.22473562  1.22473562  1.22473562  1.22473562]]\n",
            "Output mean per feature: [0. 0. 0. 0.]\n",
            "Output std per feature: [0.99999244 0.99999244 0.99999244 0.99999244]\n",
            "\n",
            "Running mean: [1.79999995 2.69999981 3.5999999  4.5       ]\n",
            "Running var: [0.70000002 0.70000002 0.70000002 0.70000002]\n",
            "\n",
            "Gradient input shape: (3, 4)\n",
            "Gradient gamma shape: (4,)\n",
            "Gradient beta shape: (4,)\n",
            "\n",
            "Inference mode output mean: [0.23904407 0.35856624 0.47808813 0.59761003]\n",
            "✅ BatchNorm тест пройден успешно!\n"
          ]
        }
      ],
      "source": [
        "# Тест BatchNorm (запустите после реализации BatchNorm)\n",
        "# print(\"⚠️ Реализуйте BatchNorm класс выше, затем раскомментируйте этот код для тестирования\")\n",
        "\n",
        "batch_norm = BatchNorm(num_features=4)\n",
        "\n",
        "# Проверим инициализацию\n",
        "assert np.allclose(batch_norm.gamma, 1.0), \"Gamma должно инициализироваться единицами\"\n",
        "assert np.allclose(batch_norm.beta, 0.0), \"Beta должно инициализироваться нулями\"\n",
        "assert np.allclose(batch_norm.running_mean, 0.0), \"Running mean должно инициализироваться нулями\"\n",
        "assert np.allclose(batch_norm.running_var, 1.0), \"Running var должно инициализироваться единицами\"\n",
        "\n",
        "print(f\"Gamma: {batch_norm.gamma}\")\n",
        "print(f\"Beta: {batch_norm.beta}\")\n",
        "\n",
        "# Тестовые данные с известной статистикой\n",
        "x_test = np.array([\n",
        "    [1, 2, 3, 4],\n",
        "    [2, 3, 4, 5],\n",
        "    [3, 4, 5, 6]\n",
        "], dtype=np.float32)\n",
        "\n",
        "print(f\"Input: \\n{x_test}\")\n",
        "print(f\"Input mean per feature: {x_test.mean(axis=0)}\")\n",
        "print(f\"Input std per feature: {x_test.std(axis=0)}\")\n",
        "\n",
        "# Forward pass в режиме обучения\n",
        "batch_norm.train()\n",
        "output = batch_norm.forward(x_test)\n",
        "\n",
        "print(f\"\\nOutput: \\n{output}\")\n",
        "print(f\"Output mean per feature: {output.mean(axis=0)}\")\n",
        "print(f\"Output std per feature: {output.std(axis=0)}\")\n",
        "\n",
        "# Проверим, что выход нормализован (среднее ≈ 0, std ≈ 1)\n",
        "assert np.allclose(output.mean(axis=0), 0, atol=1e-6), \"Среднее должно быть близко к 0\"\n",
        "assert np.allclose(output.std(axis=0), 1, atol=1e-6), \"Стандартное отклонение должно быть близко к 1\"\n",
        "\n",
        "# Проверим обновление running статистики\n",
        "print(f\"\\nRunning mean: {batch_norm.running_mean}\")\n",
        "print(f\"Running var: {batch_norm.running_var}\")\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.ones_like(output)\n",
        "grad_input = batch_norm.backward(grad_output)\n",
        "\n",
        "print(f\"\\nGradient input shape: {grad_input.shape}\")\n",
        "print(f\"Gradient gamma shape: {batch_norm.grad_gamma.shape}\")\n",
        "print(f\"Gradient beta shape: {batch_norm.grad_beta.shape}\")\n",
        "\n",
        "assert grad_input.shape == x_test.shape, \"Неправильная форма градиента по входу\"\n",
        "assert batch_norm.grad_gamma.shape == batch_norm.gamma.shape, \"Неправильная форма градиента gamma\"\n",
        "assert batch_norm.grad_beta.shape == batch_norm.beta.shape, \"Неправильная форма градиента beta\"\n",
        "\n",
        "# Тест режима инференса\n",
        "batch_norm.eval()\n",
        "output_eval = batch_norm.forward(x_test)\n",
        "print(f\"\\nInference mode output mean: {output_eval.mean(axis=0)}\")\n",
        "\n",
        "print(\"✅ BatchNorm тест пройден успешно!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zr9F4b7Nw1E"
      },
      "source": [
        "## 8. Adam Оптимизатор\n",
        "\n",
        "### Теория\n",
        "\n",
        "**Adam (Adaptive Moment Estimation)** - современный оптимизатор, сочетающий преимущества RMSprop и Momentum.\n",
        "\n",
        "**Принцип работы:**\n",
        "1. Вычисление экспоненциального скользящего среднего градиентов (momentum)\n",
        "2. Вычисление экспоненциального скользящего среднего квадратов градиентов (RMSprop)\n",
        "3. Коррекция смещения (bias correction)\n",
        "4. Обновление параметров\n",
        "\n",
        "**Преимущества:**\n",
        "- Адаптивные learning rate для каждого параметра\n",
        "- Хорошо работает на практике\n",
        "- Требует мало настройки гиперпараметров\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "-W82cQkUNw1E"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "\n",
        "        # Словари для хранения моментов для каждого слоя\n",
        "        self.m = {}  # first moment\n",
        "        self.v = {}  # second moment\n",
        "        self.t = 0   # time step\n",
        "\n",
        "    def update(self, layer, layer_id):\n",
        "        \"\"\"\n",
        "        Обновление параметров слоя с помощью Adam\n",
        "\n",
        "        Args:\n",
        "            layer: слой с градиентами\n",
        "            layer_id: уникальный идентификатор слоя\n",
        "        \"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        # TODO: Обновите веса, если есть градиенты\n",
        "        if hasattr(layer, 'grad_weight') and layer.grad_weight is not None:\n",
        "            grad_w = layer.grad_weight\n",
        "            # TODO: Инициализируйте моменты для весов\n",
        "            if f\"{layer_id}_weight\" not in self.m:\n",
        "                self.m[f\"{layer_id}_weight\"] = np.zeros_like(layer.weight)\n",
        "                self.v[f\"{layer_id}_weight\"] = np.zeros_like(layer.weight)\n",
        "\n",
        "            # TODO: Обновите первый момент (momentum)\n",
        "            self.m[f\"{layer_id}_weight\"] = (self.beta1 * self.m[f\"{layer_id}_weight\"] + (1 - self.beta1) * grad_w)\n",
        "\n",
        "            # TODO: Обновите второй момент (RMSprop)\n",
        "            self.v[f\"{layer_id}_weight\"] = (self.beta2 * self.v[f\"{layer_id}_weight\"] + (1 - self.beta2) * (grad_w ** 2))\n",
        "\n",
        "            # TODO: Коррекция смещения\n",
        "            m_corrected = self.m[f\"{layer_id}_weight\"] / (1 - self.beta1 ** self.t)\n",
        "            v_corrected = self.v[f\"{layer_id}_weight\"] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            # TODO: Обновите веса\n",
        "            layer.weight -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.eps)\n",
        "\n",
        "        # TODO: Обновите bias аналогично весам\n",
        "        if hasattr(layer, 'grad_bias') and layer.grad_bias is not None:\n",
        "          grad_b = layer.grad_bias\n",
        "          if f\"{layer_id}_bias\" not in self.m:\n",
        "              self.m[f\"{layer_id}_bias\"] = np.zeros_like(layer.bias)\n",
        "              self.v[f\"{layer_id}_bias\"] = np.zeros_like(layer.bias)\n",
        "\n",
        "          # TODO: Реализуйте обновление bias\n",
        "          # Обновляем первый момент (momentum)\n",
        "          self.m[f\"{layer_id}_bias\"] = (self.beta1 * self.m[f\"{layer_id}_bias\"] +\n",
        "                                        (1 - self.beta1) * grad_b)\n",
        "          # Обновляем второй момент (RMSprop)\n",
        "          self.v[f\"{layer_id}_bias\"] = (self.beta2 * self.v[f\"{layer_id}_bias\"] +\n",
        "                                        (1 - self.beta2) * (grad_b ** 2))\n",
        "\n",
        "          # Коррекция смещения\n",
        "          m_corrected_b = self.m[f\"{layer_id}_bias\"] / (1 - self.beta1 ** self.t)\n",
        "          v_corrected_b = self.v[f\"{layer_id}_bias\"] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "          layer.bias -= self.learning_rate * m_corrected_b / (np.sqrt(v_corrected_b) + self.eps)\n",
        "\n",
        "    def zero_grad(self, layers):\n",
        "        \"\"\"\n",
        "        Обнуление градиентов\n",
        "        \"\"\"\n",
        "        for layer in layers:\n",
        "            if hasattr(layer, 'grad_weight'):\n",
        "                layer.grad_weight = None\n",
        "            if hasattr(layer, 'grad_bias'):\n",
        "                layer.grad_bias = None\n",
        "            if hasattr(layer, 'grad_gamma'):\n",
        "                layer.grad_gamma = None\n",
        "            if hasattr(layer, 'grad_beta'):\n",
        "                layer.grad_beta = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XMNmrthKNw1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e79158d-3943-45a7-bd04-7cab83d73dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weight: \n",
            "[[-0.25727419  1.13583   ]\n",
            " [-1.18942608 -0.06203191]\n",
            " [ 1.23175618  0.12678166]]\n",
            "Initial bias: [0. 0.]\n",
            "Weight gradient: \n",
            "[[0.1 0.2]\n",
            " [0.3 0.4]\n",
            " [0.5 0.6]]\n",
            "Bias gradient: [0.1 0.2]\n",
            "\n",
            "After 1 step:\n",
            "Updated weight: \n",
            "[[-0.26727419  1.12583   ]\n",
            " [-1.19942608 -0.07203191]\n",
            " [ 1.22175618  0.11678166]]\n",
            "Updated bias: [-0.01 -0.01]\n",
            "Time step: 1\n",
            "✅ Adam тест пройден успешно!\n"
          ]
        }
      ],
      "source": [
        "# Тест Adam (запустите после реализации Adam)\n",
        "# print(\"⚠️ Реализуйте Adam класс выше, затем раскомментируйте этот код для тестирования\")\n",
        "\n",
        "# Создание тестового слоя\n",
        "layer = Linear(3, 2)\n",
        "adam = Adam(learning_rate=0.01)\n",
        "\n",
        "# Создание фиктивных градиентов\n",
        "layer.grad_weight = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype=np.float32)\n",
        "layer.grad_bias = np.array([0.1, 0.2], dtype=np.float32)\n",
        "\n",
        "# Сохранение начальных весов\n",
        "initial_weight = layer.weight.copy()\n",
        "initial_bias = layer.bias.copy()\n",
        "\n",
        "print(f\"Initial weight: \\n{initial_weight}\")\n",
        "print(f\"Initial bias: {initial_bias}\")\n",
        "print(f\"Weight gradient: \\n{layer.grad_weight}\")\n",
        "print(f\"Bias gradient: {layer.grad_bias}\")\n",
        "\n",
        "# Проверим инициализацию Adam\n",
        "assert len(adam.m) == 0, \"Моменты должны быть пустыми при инициализации\"\n",
        "assert len(adam.v) == 0, \"Моменты должны быть пустыми при инициализации\"\n",
        "assert adam.t == 0, \"Time step должен быть равен 0\"\n",
        "\n",
        "# Выполним один шаг оптимизации\n",
        "adam.update(layer, \"test_layer\")\n",
        "\n",
        "print(f\"\\nAfter 1 step:\")\n",
        "print(f\"Updated weight: \\n{layer.weight}\")\n",
        "print(f\"Updated bias: {layer.bias}\")\n",
        "print(f\"Time step: {adam.t}\")\n",
        "\n",
        "# Проверим, что веса изменились\n",
        "assert not np.allclose(layer.weight, initial_weight), \"Веса должны измениться после обновления\"\n",
        "assert not np.allclose(layer.bias, initial_bias), \"Bias должен измениться после обновления\"\n",
        "\n",
        "# Проверим, что моменты инициализированы\n",
        "assert \"test_layer_weight\" in adam.m, \"Момент для весов должен быть создан\"\n",
        "assert \"test_layer_bias\" in adam.m, \"Момент для bias должен быть создан\"\n",
        "assert \"test_layer_weight\" in adam.v, \"Момент для весов должен быть создан\"\n",
        "assert \"test_layer_bias\" in adam.v, \"Момент для bias должен быть создан\"\n",
        "\n",
        "# Проверим формы моментов\n",
        "assert adam.m[\"test_layer_weight\"].shape == layer.weight.shape, \"Неправильная форма момента весов\"\n",
        "assert adam.m[\"test_layer_bias\"].shape == layer.bias.shape, \"Неправильная форма момента bias\"\n",
        "\n",
        "# Тест zero_grad\n",
        "adam.zero_grad([layer])\n",
        "assert layer.grad_weight is None, \"Градиенты весов должны быть обнулены\"\n",
        "assert layer.grad_bias is None, \"Градиенты bias должны быть обнулены\"\n",
        "\n",
        "print(\"✅ Adam тест пройден успешно!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT1q-eDlNw1E"
      },
      "source": [
        "## 9. Функции потерь\n",
        "\n",
        "### Теория\n",
        "\n",
        "**Функции потерь** измеряют разность между предсказаниями модели и истинными значениями.\n",
        "\n",
        "**Cross-Entropy Loss:**\n",
        "- Используется для задач классификации\n",
        "- Формула: `-log(p_correct_class)`\n",
        "- Штрафует неверные предсказания экспоненциально\n",
        "\n",
        "**Mean Squared Error (MSE):**\n",
        "- Используется для задач регрессии\n",
        "- Формула: `(y_pred - y_true)²`\n",
        "- Чувствителен к выбросам\n",
        "\n",
        "### Реализация\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "3q9GfngRNw1E"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss:\n",
        "    def __init__(self):\n",
        "        self.predictions = None\n",
        "        self.targets = None\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Вычисление Cross-Entropy Loss\n",
        "\n",
        "        Args:\n",
        "            predictions: предсказания модели (batch_size, num_classes)\n",
        "            targets: истинные метки класса (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            значение функции потерь\n",
        "        \"\"\"\n",
        "        self.predictions = predictions\n",
        "        self.targets = targets\n",
        "\n",
        "        # TODO: Примените softmax к предсказаниям\n",
        "        self.softmax_pred = softmax(predictions)\n",
        "\n",
        "        # TODO: Вычислите cross-entropy loss\n",
        "        batch_size = predictions.shape[0]\n",
        "\n",
        "        correct_class_probs = self.softmax_pred[np.arange(batch_size), targets]\n",
        "        loss = -np.mean(np.log(correct_class_probs + 1e-15))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Вычисление градиента Cross-Entropy Loss\n",
        "\n",
        "        Returns:\n",
        "            градиент по предсказаниям\n",
        "        \"\"\"\n",
        "        # TODO: Вычислите градиент\n",
        "        batch_size = self.predictions.shape[0]\n",
        "        num_classes = self.predictions.shape[1]\n",
        "\n",
        "        # Создаем one-hot кодировку для целей\n",
        "        one_hot_targets = one_hot_encode(self.targets, num_classes)\n",
        "\n",
        "        # Градиент: (softmax_pred - one_hot_targets) / batch_size\n",
        "        grad = (self.softmax_pred - one_hot_targets) / batch_size\n",
        "\n",
        "        return grad\n",
        "\n",
        "\n",
        "class MSELoss:\n",
        "    def __init__(self):\n",
        "        self.predictions = None\n",
        "        self.targets = None\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Вычисление Mean Squared Error\n",
        "\n",
        "        Args:\n",
        "            predictions: предсказания модели\n",
        "            targets: истинные значения\n",
        "\n",
        "        Returns:\n",
        "            значение функции потерь\n",
        "        \"\"\"\n",
        "        self.predictions = predictions\n",
        "        self.targets = targets\n",
        "\n",
        "        # TODO: Вычислите MSE\n",
        "        loss = np.mean((predictions - targets) ** 2)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Вычисление градиента MSE\n",
        "\n",
        "        Returns:\n",
        "            градиент по предсказаниям\n",
        "        \"\"\"\n",
        "        # TODO: Вычислите градиент MSE\n",
        "        batch_size = self.predictions.shape[0]\n",
        "        grad = 2 * (self.predictions - self.targets) / batch_size\n",
        "\n",
        "        return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Устойчивая реализация softmax\n",
        "    \"\"\"\n",
        "    # TODO: Реализуйте softmax функцию\n",
        "    x_stable = x - np.max(x, axis=1, keepdims=True)\n",
        "    exp_x = np.exp(x_stable)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    \"\"\"\n",
        "    Преобразование меток в one-hot кодировку\n",
        "    \"\"\"\n",
        "    # TODO: Создайте one-hot кодировку\n",
        "    batch_size = labels.shape[0]\n",
        "    one_hot = np.zeros((batch_size, num_classes))\n",
        "    one_hot[np.arange(batch_size), labels] = 1\n",
        "    return one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "EdWXYCbRNw1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35228e3f-48af-48f8-d802-a1b0a0ca4b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Тестирование CrossEntropyLoss...\n",
            "Predictions: \n",
            "[[2.  1.  0.1]\n",
            " [1.  3.  0.2]]\n",
            "Targets: [0 1]\n",
            "CrossEntropy Loss: 0.2981\n",
            "Gradient shape: (2, 3)\n",
            "Gradient: \n",
            "[[-0.17049944  0.12121648  0.04928295]\n",
            " [ 0.05657142 -0.08199063  0.02541918]]\n",
            "✅ CrossEntropyLoss тест пройден!\n",
            "\n",
            "📊 Тестирование MSELoss...\n",
            "Predictions: \n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "Targets: \n",
            "[[1.5 2.5]\n",
            " [2.5 3.5]]\n",
            "MSE Loss: 0.2500\n",
            "MSE Gradient shape: (2, 2)\n",
            "MSE Gradient: \n",
            "[[-0.5 -0.5]\n",
            " [ 0.5  0.5]]\n",
            "✅ MSELoss тест пройден!\n",
            "\n",
            "🎯 Тестирование Softmax...\n",
            "Input: \n",
            "[[1. 2. 3.]\n",
            " [1. 1. 1.]]\n",
            "Softmax output: \n",
            "[[0.09003057 0.24472846 0.66524094]\n",
            " [0.33333334 0.33333334 0.33333334]]\n",
            "✅ Softmax тест пройден!\n",
            "\n",
            "🏷️ Тестирование One-hot encoding...\n",
            "Labels: [0 2 1 0]\n",
            "One-hot: \n",
            "[[1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n",
            "✅ One-hot encoding тест пройден!\n",
            "\n",
            "🎉 Все тесты функций потерь пройдены успешно!\n"
          ]
        }
      ],
      "source": [
        "# Тест функций потерь (запустите после реализации Loss функций)\n",
        "# print(\"⚠️ Реализуйте функции потерь выше, затем раскомментируйте этот код для тестирования\")\n",
        "\n",
        "# Тест CrossEntropyLoss\n",
        "print(\"🔥 Тестирование CrossEntropyLoss...\")\n",
        "ce_loss = CrossEntropyLoss()\n",
        "\n",
        "# Тестовые данные\n",
        "predictions = np.array([[2.0, 1.0, 0.1], [1.0, 3.0, 0.2]], dtype=np.float32)\n",
        "targets = np.array([0, 1], dtype=np.int32)\n",
        "\n",
        "print(f\"Predictions: \\n{predictions}\")\n",
        "print(f\"Targets: {targets}\")\n",
        "\n",
        "# Forward pass\n",
        "loss_value = ce_loss.forward(predictions, targets)\n",
        "print(f\"CrossEntropy Loss: {loss_value:.4f}\")\n",
        "\n",
        "# Проверим, что loss положительный\n",
        "assert loss_value > 0, \"CrossEntropy loss должен быть положительным\"\n",
        "\n",
        "# Backward pass\n",
        "grad = ce_loss.backward()\n",
        "print(f\"Gradient shape: {grad.shape}\")\n",
        "print(f\"Gradient: \\n{grad}\")\n",
        "\n",
        "# Проверим форму градиента\n",
        "assert grad.shape == predictions.shape, \"Неправильная форма градиента CrossEntropy\"\n",
        "\n",
        "# Проверим, что сумма градиентов по классам равна 0 (свойство softmax)\n",
        "assert np.allclose(grad.sum(axis=1), 0, atol=1e-6), \"Сумма градиентов по классам должна быть 0\"\n",
        "\n",
        "print(\"✅ CrossEntropyLoss тест пройден!\")\n",
        "\n",
        "# Тест MSELoss\n",
        "print(\"\\n📊 Тестирование MSELoss...\")\n",
        "mse_loss = MSELoss()\n",
        "\n",
        "# Тестовые данные для регрессии\n",
        "predictions_reg = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n",
        "targets_reg = np.array([[1.5, 2.5], [2.5, 3.5]], dtype=np.float32)\n",
        "\n",
        "print(f\"Predictions: \\n{predictions_reg}\")\n",
        "print(f\"Targets: \\n{targets_reg}\")\n",
        "\n",
        "# Forward pass\n",
        "mse_value = mse_loss.forward(predictions_reg, targets_reg)\n",
        "print(f\"MSE Loss: {mse_value:.4f}\")\n",
        "\n",
        "# Проверим, что loss положительный\n",
        "assert mse_value >= 0, \"MSE loss должен быть неотрицательным\"\n",
        "\n",
        "# Backward pass\n",
        "grad_mse = mse_loss.backward()\n",
        "print(f\"MSE Gradient shape: {grad_mse.shape}\")\n",
        "print(f\"MSE Gradient: \\n{grad_mse}\")\n",
        "\n",
        "# Проверим форму градиента\n",
        "assert grad_mse.shape == predictions_reg.shape, \"Неправильная форма градиента MSE\"\n",
        "\n",
        "print(\"✅ MSELoss тест пройден!\")\n",
        "\n",
        "# Тест softmax функции\n",
        "print(\"\\n🎯 Тестирование Softmax...\")\n",
        "x_softmax = np.array([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]], dtype=np.float32)\n",
        "softmax_output = softmax(x_softmax)\n",
        "\n",
        "print(f\"Input: \\n{x_softmax}\")\n",
        "print(f\"Softmax output: \\n{softmax_output}\")\n",
        "\n",
        "# Проверим, что сумма вероятностей равна 1\n",
        "assert np.allclose(softmax_output.sum(axis=1), 1.0), \"Сумма softmax должна быть равна 1\"\n",
        "\n",
        "# Проверим, что все значения положительные\n",
        "assert np.all(softmax_output > 0), \"Все значения softmax должны быть положительными\"\n",
        "assert np.all(softmax_output < 1), \"Все значения softmax должны быть меньше 1\"\n",
        "\n",
        "print(\"✅ Softmax тест пройден!\")\n",
        "\n",
        "# Тест one-hot encoding\n",
        "print(\"\\n🏷️ Тестирование One-hot encoding...\")\n",
        "labels = np.array([0, 2, 1, 0])\n",
        "one_hot = one_hot_encode(labels, num_classes=3)\n",
        "\n",
        "print(f\"Labels: {labels}\")\n",
        "print(f\"One-hot: \\n{one_hot}\")\n",
        "\n",
        "# Проверим форму\n",
        "assert one_hot.shape == (4, 3), \"Неправильная форма one-hot кодировки\"\n",
        "\n",
        "# Проверим, что каждая строка содержит ровно одну единицу\n",
        "assert np.all(one_hot.sum(axis=1) == 1), \"Каждая строка должна содержать ровно одну единицу\"\n",
        "\n",
        "print(\"✅ One-hot encoding тест пройден!\")\n",
        "\n",
        "print(\"\\n🎉 Все тесты функций потерь пройдены успешно!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czv4F9nwNw1F"
      },
      "source": [
        "## 10. Обновленная архитектура нейронной сети\n",
        "\n",
        "Теперь создайте нейронную сеть с использованием всех реализованных компонентов."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, dataset, batch_size=32, shuffle=False, drop_last = False):\n",
        "        \"\"\"\n",
        "        Инициализация загрузчика данных\n",
        "\n",
        "        Args:\n",
        "            dataset: list или np.array, содержащий данные\n",
        "            batch_size: размер батча\n",
        "            shuffle: перемешивать ли данные\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.drop_last = drop_last\n",
        "\n",
        "        self._shuffle_data()\n",
        "\n",
        "    def _shuffle_data(self):\n",
        "        \"\"\"Перемешивание данных\"\"\"\n",
        "        self.array = list(range(self.dataset))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.array)\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Инициализация итератора\"\"\"\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Получение следующего батча (векторы + метки)\"\"\"\n",
        "        if len(self.array) == 0:\n",
        "            self._shuffle_data()\n",
        "            raise StopIteration()\n",
        "\n",
        "        if len(self.array) < self.batch_size and self.drop_last:\n",
        "            self._shuffle_data()\n",
        "            raise StopIteration()\n",
        "\n",
        "        if len(self.array) > self.batch_size:\n",
        "            selected = self.array[:self.batch_size]\n",
        "            self.array = self.array[self.batch_size:]\n",
        "        else:\n",
        "            selected = self.array\n",
        "            self.array = []\n",
        "\n",
        "        data = [self.dataset[ind][0] for ind in selected]\n",
        "        labels = [self.dataset[ind][1] for ind in selected]\n",
        "        return np.array(data, dtype=np.float32), np.array(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Количество батчей в датасете\"\"\"\n",
        "        return int(np.ceil(len(self.dataset/ self.batch_size)))\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"Получение всего датасета\"\"\"\n",
        "        return self.dataset\n",
        ""
      ],
      "metadata": {
        "id": "jFPCj9vEiLQI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "f0hxt8lANw1F"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, batch_size=128, epochs=10):\n",
        "        # TODO: Создайте архитектуру нейронной сети\n",
        "        self.model = Sequential(\n",
        "\n",
        "            Linear(784,512),\n",
        "            BatchNorm(512),\n",
        "            ReLU(),\n",
        "            Dropout(0.4),\n",
        "\n",
        "            Linear(512,256),\n",
        "            BatchNorm(256),\n",
        "            ReLU(),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Linear(256,128),\n",
        "            BatchNorm(128),\n",
        "            ReLU(),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Linear(128,10)\n",
        "        )\n",
        "        self.optimizer = Adam(self.model.parameters())\n",
        "        self.loss_func = CrossEntropyLoss\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model.forward(x)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        return self.model.backward(grad_output)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_trainable_layers(self):\n",
        "        \"\"\"\n",
        "        Получение всех слоев с обучаемыми параметрами\n",
        "        \"\"\"\n",
        "        trainable_layers = []\n",
        "        for layer in self.model.layers:\n",
        "            if hasattr(layer, 'update_weights'):\n",
        "                trainable_layers.append(layer)\n",
        "        return trainable_layers\n",
        "\n",
        "    def compute_loss(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Вычисление функции потерь\n",
        "        \"\"\"\n",
        "        return self.loss_func.forward(predictions, targets)\n",
        "\n",
        "    def compute_grad_loss(self):\n",
        "        \"\"\"\n",
        "        Вычисление градиента функции потерь\n",
        "        \"\"\"\n",
        "        return self.loss_fn.backward()\n",
        "\n",
        "    def train_model(self, dataset):\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
        "      for epoch in range(self.epochs):\n",
        "        print(f\"Epoch: {epoch + 1} of {self.epochs}\")\n",
        "        for x_batch, y_batch in tqdm(dataloader):\n",
        "          self.optimizer.zero_grad()\n",
        "          predictions = self.model.forward(x_batch)\n",
        "          loss = self.loss_func(predictions, y_batch)\n",
        "          loss.backward()\n",
        "          self.optimizer.update()\n",
        "      return self\n",
        "\n",
        "    def predict_model(self, dataset):\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False)\n",
        "      predictions = []\n",
        "      for x_batch, y_batch in dataloader:\n",
        "        result = self.model.forward(x_batch)\n",
        "        batch_predictions = np.argmax(result.array, axis=1)\n",
        "        predictions.extend(batch_predictions)\n",
        "      return np.array(predictions)\n",
        "\n",
        "\n",
        "# TODO: Создайте экземпляр сети\n",
        "net = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Синтетический датасет\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "X = np.random.randn(num_samples, 784)  # 1000 изображений 28x28\n",
        "y = np.random.randint(0, 10, num_samples)  # 10 классов\n",
        "dataset = (X, y)"
      ],
      "metadata": {
        "id": "mxYHNP9Gtiwb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация и обучение\n",
        "nn = NeuralNetwork(batch_size=128, epochs=20)\n",
        "nn.train_model(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MIsyybEruSsA",
        "outputId": "27f6ac70-5339-488e-a4e0-9c2297f28d54"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'tuple' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2063195112.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Инициализация и обучение\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4086256223.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch + 1} of {self.epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2505752269.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, drop_last)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_shuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2505752269.py\u001b[0m in \u001b[0;36m_shuffle_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_shuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m\"\"\"Перемешивание данных\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}