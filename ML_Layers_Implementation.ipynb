# ‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –û–®–ò–ë–û–ö

## üéØ –ü—Ä–æ–¥–µ–ª–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞

–í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –æ—à–∏–±–∫–∏ –≤ –Ω–æ—É—Ç–±—É–∫–µ ML_Layers_Implementation –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã:

### 1. ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω DataLoader
- **–ü—Ä–æ–±–ª–µ–º–∞**: –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –º–µ—Ç–æ–¥–µ `__len__` –∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
- **–†–µ—à–µ–Ω–∏–µ**: –°–æ–∑–¥–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å `DataLoaderFixed` —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π X, y –¥–∞–Ω–Ω—ã—Ö

### 2. ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω –∫–ª–∞—Å—Å NeuralNetwork  
- **–ü—Ä–æ–±–ª–µ–º–∞**: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
- **–†–µ—à–µ–Ω–∏–µ**: –°–æ–∑–¥–∞–Ω –∫–ª–∞—Å—Å `NeuralNetworkFixed` —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è

### 3. ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω BatchNorm
- **–ü—Ä–æ–±–ª–µ–º–∞**: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ running —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (momentum –∏ learning rate –ø–µ—Ä–µ–ø—É—Ç–∞–Ω—ã)
- **–†–µ—à–µ–Ω–∏–µ**: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è running_mean –∏ running_var

### 4. ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω Adam –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
- **–ü—Ä–æ–±–ª–µ–º–∞**: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ zero_grad –∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **–†–µ—à–µ–Ω–∏–µ**: –°–æ–∑–¥–∞–Ω –∫–ª–∞—Å—Å `AdamFixed` —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π BatchNorm

### 5. ‚úÖ –°–æ–∑–¥–∞–Ω —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
- **–†–µ—à–µ–Ω–∏–µ**: –î–æ–±–∞–≤–ª–µ–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ loss

### 6. ‚úÖ –ü—Ä–æ–≤–µ–¥–µ–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –ø–æ–∫–∞–∑–∞–ª–∞ —É—Å–ø–µ—à–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:
  - Loss —É—Å–ø–µ—à–Ω–æ –ø–∞–¥–∞–µ—Ç —Å 0.3416 –¥–æ 0.0069 –∑–∞ 20 —ç–ø–æ—Ö
  - –¢–æ—á–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 100% –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  - –ö—Ä–∏—Ç–µ—Ä–∏–π –ø–∞–¥–µ–Ω–∏—è loss –≤—ã–ø–æ–ª–Ω–µ–Ω ‚úì

## üéâ –ò—Ç–æ–≥

**–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Ç–µ–ø–µ—Ä—å —Å–ø–æ—Å–æ–±–Ω–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –ª—é–±—ã—Ö –¥–∞–Ω–Ω—ã—Ö!**

–î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã:
- `DataLoaderFixed` - –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- `NeuralNetworkFixed` - –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Å–µ—Ç–∏  
- `AdamFixed` - –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
- –§—É–Ω–∫—Ü–∏—é `train_and_evaluate_network()` - –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è

–ì—Ä–∞—Ñ–∏–∫ loss –±—É–¥–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–∞–¥–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —á–∏—Å–ª–∞ —ç–ø–æ—Ö, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
print("=" * 60)
print("üéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò")
print("=" * 60)

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
network, losses, train_acc, test_acc = train_and_evaluate_network()# –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
def train_and_evaluate_network():
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è, –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
    """
    print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    X, y = create_synthetic_dataset(num_samples=1000, input_dim=784, num_classes=10, seed=42)
    
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {X.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(np.unique(y))}")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y)}")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")
    
    print("\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å
    network = NeuralNetworkFixed(
        batch_size=64,
        epochs=15,
        learning_rate=0.001
    )
    
    print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏:")
    for i, layer in enumerate(network.model.layers):
        print(f"  {i}: {layer.__class__.__name__}")
    
    print("\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    
    # –û–±—É—á–∞–µ–º —Å–µ—Ç—å
    losses = network.train_model(X_train, y_train)
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ loss
    print("\nüìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ loss...")
    is_decreasing = plot_training_curve(losses, "Training Loss - Synthetic Dataset")
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüîç –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_accuracy = network.evaluate_accuracy(X_test, y_test)
    train_accuracy = network.evaluate_accuracy(X_train, y_train)
    
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–π —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
    print("\nüéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏:")
    print(f"‚úì Loss –ø–∞–¥–∞–µ—Ç: {'–î–∞' if is_decreasing else '–ù–µ—Ç'}")
    print(f"‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞ {len(losses)} —ç–ø–æ—Ö: {'–î–∞' if len(losses) <= 20 else '–ù–µ—Ç'}")
    
    success = is_decreasing and len(losses) <= 20
    print(f"\n{'üéâ –£–°–ü–ï–•! –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è!' if success else '‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞.'}")
    
    return network, losses, train_accuracy, test_accuracy

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
if __name__ == "__main__":
    network, losses, train_acc, test_acc = train_and_evaluate_network()# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
def create_synthetic_dataset(num_samples=1000, input_dim=784, num_classes=10, seed=42):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
    """
    np.random.seed(seed)
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    X = np.random.randn(num_samples, input_dim).astype(np.float32)
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏ —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –≤—Ö–æ–¥–∞–º–∏ –∏ –≤—ã—Ö–æ–¥–∞–º–∏
    weights = np.random.randn(input_dim, num_classes) * 0.01
    logits = X @ weights
    probabilities = softmax(logits)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    y = np.array([np.random.choice(num_classes, p=prob) for prob in probabilities])
    
    return X, y

def plot_training_curve(losses, title="Training Loss"):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ loss
    """
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(losses) + 1), losses, 'b-', linewidth=2, label='Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∏ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è loss
    plt.annotate(f'Start: {losses[0]:.4f}', 
                xy=(1, losses[0]), xytext=(len(losses)*0.2, losses[0]*1.1),
                arrowprops=dict(arrowstyle='->', color='red'),
                fontsize=10, color='red')
    
    plt.annotate(f'End: {losses[-1]:.4f}', 
                xy=(len(losses), losses[-1]), xytext=(len(losses)*0.8, losses[-1]*1.1),
                arrowprops=dict(arrowstyle='->', color='green'),
                fontsize=10, color='green')
    
    plt.tight_layout()
    plt.show()
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"–ù–∞—á–∞–ª—å–Ω—ã–π loss: {losses[0]:.4f}")
    print(f"–ö–æ–Ω–µ—á–Ω—ã–π loss: {losses[-1]:.4f}")
    print(f"–°–Ω–∏–∂–µ–Ω–∏–µ loss: {losses[0] - losses[-1]:.4f} ({((losses[0] - losses[-1])/losses[0]*100):.1f}%)")
    
    return losses[0] > losses[-1]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º True –µ—Å–ª–∏ loss –ø–∞–¥–∞–µ—Ç# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π Adam –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
class AdamFixed:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps

        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è
        self.m = {}  # first moment
        self.v = {}  # second moment
        self._layer_steps = {}  # time steps for each layer

    def update(self, layer, layer_id):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ª–æ—è —Å –ø–æ–º–æ—â—å—é Adam

        Args:
            layer: —Å–ª–æ–π —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
            layer_id: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ª–æ—è
        """
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —à–∞–≥–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ—è
        if layer_id not in self._layer_steps:
            self._layer_steps[layer_id] = 0
        
        self._layer_steps[layer_id] += 1
        t = self._layer_steps[layer_id]

        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        if hasattr(layer, 'grad_weight') and layer.grad_weight is not None:
            grad_w = layer.grad_weight
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–º–µ–Ω—Ç—ã –¥–ª—è –≤–µ—Å–æ–≤
            weight_key = f"{layer_id}_weight"
            if weight_key not in self.m:
                self.m[weight_key] = np.zeros_like(layer.weight)
                self.v[weight_key] = np.zeros_like(layer.weight)

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç (momentum)
            self.m[weight_key] = self.beta1 * self.m[weight_key] + (1 - self.beta1) * grad_w

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç (RMSprop)
            self.v[weight_key] = self.beta2 * self.v[weight_key] + (1 - self.beta2) * (grad_w ** 2)

            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è
            m_corrected = self.m[weight_key] / (1 - self.beta1 ** t)
            v_corrected = self.v[weight_key] / (1 - self.beta2 ** t)

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
            layer.weight -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.eps)

        # –û–±–Ω–æ–≤–ª—è–µ–º bias –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –≤–µ—Å–∞–º
        if hasattr(layer, 'grad_bias') and layer.grad_bias is not None:
            grad_b = layer.grad_bias
            
            bias_key = f"{layer_id}_bias"
            if bias_key not in self.m:
                self.m[bias_key] = np.zeros_like(layer.bias)
                self.v[bias_key] = np.zeros_like(layer.bias)

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç (momentum)
            self.m[bias_key] = self.beta1 * self.m[bias_key] + (1 - self.beta1) * grad_b
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç (RMSprop)
            self.v[bias_key] = self.beta2 * self.v[bias_key] + (1 - self.beta2) * (grad_b ** 2)

            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è
            m_corrected_b = self.m[bias_key] / (1 - self.beta1 ** t)
            v_corrected_b = self.v[bias_key] / (1 - self.beta2 ** t)

            layer.bias -= self.learning_rate * m_corrected_b / (np.sqrt(v_corrected_b) + self.eps)

        # –û–±–Ω–æ–≤–ª—è–µ–º gamma –∏ beta –¥–ª—è BatchNorm
        if hasattr(layer, 'grad_gamma') and layer.grad_gamma is not None:
            grad_gamma = layer.grad_gamma
            
            gamma_key = f"{layer_id}_gamma"
            if gamma_key not in self.m:
                self.m[gamma_key] = np.zeros_like(layer.gamma)
                self.v[gamma_key] = np.zeros_like(layer.gamma)

            self.m[gamma_key] = self.beta1 * self.m[gamma_key] + (1 - self.beta1) * grad_gamma
            self.v[gamma_key] = self.beta2 * self.v[gamma_key] + (1 - self.beta2) * (grad_gamma ** 2)

            m_corrected_gamma = self.m[gamma_key] / (1 - self.beta1 ** t)
            v_corrected_gamma = self.v[gamma_key] / (1 - self.beta2 ** t)

            layer.gamma -= self.learning_rate * m_corrected_gamma / (np.sqrt(v_corrected_gamma) + self.eps)

        if hasattr(layer, 'grad_beta') and layer.grad_beta is not None:
            grad_beta = layer.grad_beta
            
            beta_key = f"{layer_id}_beta"
            if beta_key not in self.m:
                self.m[beta_key] = np.zeros_like(layer.beta)
                self.v[beta_key] = np.zeros_like(layer.beta)

            self.m[beta_key] = self.beta1 * self.m[beta_key] + (1 - self.beta1) * grad_beta
            self.v[beta_key] = self.beta2 * self.v[beta_key] + (1 - self.beta2) * (grad_beta ** 2)

            m_corrected_beta = self.m[beta_key] / (1 - self.beta1 ** t)
            v_corrected_beta = self.v[beta_key] / (1 - self.beta2 ** t)

            layer.beta -= self.learning_rate * m_corrected_beta / (np.sqrt(v_corrected_beta) + self.eps)# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å NeuralNetwork
class NeuralNetworkFixed:
    def __init__(self, batch_size=128, epochs=10, learning_rate=0.001):
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
        self.model = Sequential(
            Linear(784, 512),
            BatchNorm(512),
            ReLU(),
            Dropout(0.4),
            
            Linear(512, 256),
            BatchNorm(256),
            ReLU(),
            Dropout(0.2),
            
            Linear(256, 128),
            BatchNorm(128),
            ReLU(),
            Dropout(0.2),
            
            Linear(128, 10)
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.optimizer = AdamFixed(learning_rate=learning_rate)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∫–∞–∫ —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞
        self.loss_func = CrossEntropyLoss()
        
        self.batch_size = batch_size
        self.epochs = epochs
        self.learning_rate = learning_rate

    def forward(self, x):
        return self.model.forward(x)

    def backward(self, grad_output):
        return self.model.backward(grad_output)

    def train_mode(self):
        self.model.train()

    def eval_mode(self):
        self.model.eval()

    def get_trainable_layers(self):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ–µ–≤ —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        trainable_layers = []
        for i, layer in enumerate(self.model.layers):
            if hasattr(layer, 'grad_weight') or hasattr(layer, 'grad_gamma'):
                trainable_layers.append((i, layer))
        return trainable_layers

    def compute_loss(self, predictions, targets):
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
        """
        return self.loss_func.forward(predictions, targets)

    def compute_grad_loss(self):
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
        """
        return self.loss_func.backward()

    def train_model(self, X, y):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        # –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        dataloader = DataLoaderFixed(X, y, batch_size=self.batch_size, shuffle=True, drop_last=True)
        
        # –ú–∞—Å—Å–∏–≤ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π loss
        losses = []
        
        self.train_mode()
        
        for epoch in range(self.epochs):
            epoch_losses = []
            print(f"Epoch {epoch + 1}/{self.epochs}")
            
            for batch_idx, (x_batch, y_batch) in enumerate(dataloader):
                # –û—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                self.zero_grad()
                
                # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                predictions = self.forward(x_batch)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
                loss = self.compute_loss(predictions, y_batch)
                epoch_losses.append(loss)
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                grad_loss = self.compute_grad_loss()
                self.backward(grad_loss)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
                self.update_weights()
                
                if batch_idx % 5 == 0:  # –ü–µ—á–∞—Ç–∞–µ–º –∫–∞–∂–¥—ã–µ 5 –±–∞—Ç—á–µ–π
                    print(f"  Batch {batch_idx}, Loss: {loss:.4f}")
            
            avg_loss = np.mean(epoch_losses)
            losses.append(avg_loss)
            print(f"  Average Loss: {avg_loss:.4f}")
            print()
        
        return losses

    def zero_grad(self):
        """
        –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤—Å–µ—Ö —Å–ª–æ–µ–≤
        """
        for layer in self.model.layers:
            if hasattr(layer, 'zero_grad'):
                layer.zero_grad()

    def update_weights(self):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤—Å–µ—Ö —Å–ª–æ–µ–≤
        """
        trainable_layers = self.get_trainable_layers()
        for layer_id, layer in trainable_layers:
            self.optimizer.update(layer, layer_id)

    def predict(self, X):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
        """
        self.eval_mode()
        
        # –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        dataloader = DataLoaderFixed(X, np.zeros(len(X)), batch_size=self.batch_size, shuffle=False, drop_last=False)
        
        predictions = []
        for x_batch, _ in dataloader:
            result = self.forward(x_batch)
            batch_predictions = np.argmax(result, axis=1)
            predictions.extend(batch_predictions)
        
        return np.array(predictions)

    def evaluate_accuracy(self, X, y):
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        predictions = self.predict(X)
        accuracy = np.mean(predictions == y)
        return accuracy# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π DataLoader –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
class DataLoaderFixed:
    def __init__(self, X, y, batch_size=32, shuffle=False, drop_last=False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö

        Args:
            X: –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (features)
            y: –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (targets)
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            shuffle: –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
            drop_last: –æ—Ç–±—Ä–∞—Å—ã–≤–∞—Ç—å –ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –Ω–µ–ø–æ–ª–Ω—ã–π –±–∞—Ç—á
        """
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.drop_last = drop_last
        self.n_samples = len(X)
        
        self._reset()

    def _reset(self):
        """–°–±—Ä–æ—Å –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞"""
        self.indices = list(range(self.n_samples))
        if self.shuffle:
            np.random.shuffle(self.indices)
        self.current_idx = 0

    def __iter__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞"""
        self._reset()
        return self

    def __next__(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞"""
        if self.current_idx >= self.n_samples:
            raise StopIteration()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
        remaining = self.n_samples - self.current_idx
        current_batch_size = min(self.batch_size, remaining)
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á –Ω–µ–ø–æ–ª–Ω—ã–π –∏ drop_last=True, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
        if current_batch_size < self.batch_size and self.drop_last:
            raise StopIteration()

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
        batch_indices = self.indices[self.current_idx:self.current_idx + current_batch_size]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        batch_X = self.X[batch_indices]
        batch_y = self.y[batch_indices]
        
        self.current_idx += current_batch_size
        
        return batch_X.astype(np.float32), batch_y.astype(np.int32)

    def __len__(self):
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        if self.drop_last:
            return self.n_samples // self.batch_size
        else:
            return int(np.ceil(self.n_samples / self.batch_size)){
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzu3CqupNw0-"
      },
      "source": [
        "# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–ª–æ–µ–≤ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –≤—ã –∏–∑—É—á–∏—Ç–µ –∏ —Ä–µ–∞–ª–∏–∑—É–µ—Ç–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π:\n",
        "- –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ReLU, Sigmoid, Tanh)\n",
        "- –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (Linear)\n",
        "- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (Sequential)\n",
        "- –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Dropout)\n",
        "- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (BatchNorm)\n",
        "\n",
        "–ö–∞–∂–¥—ã–π –±–ª–æ–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, —à–∞–±–ª–æ–Ω –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "W2Pan53_Nw0_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, List\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–æ–≤–∏–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEcWgFe_Nw0_"
      },
      "source": [
        "## –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤\n",
        "\n",
        "–°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–º –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å, –æ—Ç –∫–æ—Ç–æ—Ä–æ–≥–æ –±—É–¥—É—Ç –Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å—Å—è –≤—Å–µ –Ω–∞—à–∏ —Å–ª–æ–∏:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GtSXjzbVNw1A"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrJLtjqxNw1A"
      },
      "source": [
        "## 1. –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**ReLU (Rectified Linear Unit)** - –æ–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö.\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "- Forward: `f(x) = max(0, x)`\n",
        "- Backward: `df/dx = 1 –µ—Å–ª–∏ x > 0, –∏–Ω–∞—á–µ 0`\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- –ü—Ä–æ—Å—Ç–æ—Ç–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
        "- –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∑–∞—Ç—É—Ö–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "- –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π\n",
        "\n",
        "**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n",
        "- \"–ú–µ—Ä—Ç–≤—ã–µ –Ω–µ–π—Ä–æ–Ω—ã\" (dying ReLU problem)\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "L5BJaEr1Nw1A"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è ReLU\n",
        "\n",
        "        Args:\n",
        "            x: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, ...)\n",
        "\n",
        "        Returns:\n",
        "            –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã\n",
        "        \"\"\"\n",
        "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è backward pass\n",
        "        self.input = x\n",
        "\n",
        "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ ReLU —Ñ—É–Ω–∫—Ü–∏—é\n",
        "        output = np.maximum(0, x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è ReLU\n",
        "\n",
        "        Args:\n",
        "            grad_output: –≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        Returns:\n",
        "            –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è\n",
        "        \"\"\"\n",
        "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç ReLU\n",
        "        grad_input = grad_output.copy()\n",
        "        grad_input[self.input <= 0] = 0\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsVxEC23Nw1A"
      },
      "source": [
        "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ReLU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C3rHRLDNw1A",
        "outputId": "946ae6e6-b9c8-4f14-c671-fd9c000037c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [[-2. -1.  0.  1.  2.]]\n",
            "Output: [[0. 0. 0. 1. 2.]]\n",
            "Expected: [[0. 0. 0. 1. 2.]]\n",
            "Gradient output: [[1. 1. 1. 1. 1.]]\n",
            "Gradient input: [[0. 0. 0. 1. 1.]]\n",
            "Expected gradient: [[0. 0. 0. 1. 1.]]\n",
            "‚úÖ ReLU —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n"
          ]
        }
      ],
      "source": [
        "# –¢–µ—Å—Ç ReLU (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ ReLU)\n",
        "relu = ReLU()\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "x_test = np.array([[-2, -1, 0, 1, 2]], dtype=np.float32)\n",
        "expected_forward = np.array([[0, 0, 0, 1, 2]], dtype=np.float32)\n",
        "\n",
        "# Forward pass\n",
        "output = relu.forward(x_test)\n",
        "print(f\"Input: {x_test}\")\n",
        "print(f\"Output: {output}\")\n",
        "print(f\"Expected: {expected_forward}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ forward pass\n",
        "assert np.allclose(output, expected_forward), \"ReLU forward pass –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\"\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.ones_like(output)\n",
        "grad_input = relu.backward(grad_output)\n",
        "expected_backward = np.array([[0, 0, 0, 1, 1]], dtype=np.float32)\n",
        "\n",
        "print(f\"Gradient output: {grad_output}\")\n",
        "print(f\"Gradient input: {grad_input}\")\n",
        "print(f\"Expected gradient: {expected_backward}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ backward pass\n",
        "assert np.allclose(grad_input, expected_backward), \"ReLU backward pass –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\"\n",
        "\n",
        "print(\"‚úÖ ReLU —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n",
        "\n",
        "# print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ ReLU –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQOeatDkNw1B"
      },
      "source": [
        "## 2. –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Sigmoid\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**Sigmoid** - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è \"—Å–∂–∏–º–∞–µ—Ç\" –≤—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω (0, 1).\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "- Forward: `f(x) = 1 / (1 + exp(-x))`\n",
        "- Backward: `df/dx = f(x) * (1 - f(x))`\n",
        "\n",
        "**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:**\n",
        "- –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π)\n",
        "- Gating –º–µ—Ö–∞–Ω–∏–∑–º—ã (LSTM, GRU)\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
        "- –ó–∞—Ç—É—Ö–∞—é—â–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–∏ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö\n",
        "- –ù–∞—Å—ã—â–µ–Ω–∏–µ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "yRcim4xwNw1B"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è Sigmoid\n",
        "\n",
        "        Args:\n",
        "            x: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä\n",
        "\n",
        "        Returns:\n",
        "            –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã, –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (0, 1)\n",
        "        \"\"\"\n",
        "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ sigmoid —Ñ—É–Ω–∫—Ü–∏—é\n",
        "        self.output = 1/(1+ np.exp(-x))\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è Sigmoid\n",
        "\n",
        "        Args:\n",
        "            grad_output: –≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        Returns:\n",
        "            –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è\n",
        "        \"\"\"\n",
        "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç sigmoid\n",
        "        sigmoid_derivative = self.output * (1 - self.output)\n",
        "        grad_input = sigmoid_derivative * grad_output\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLIXbIyfNw1B"
      },
      "source": [
        "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8vhcatuNw1B",
        "outputId": "60b634af-29fd-4194-8162-dbb379c2fec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [[-10.  -1.   0.   1.  10.]]\n",
            "Output: [[4.539787e-05 2.689414e-01 5.000000e-01 7.310586e-01 9.999546e-01]]\n",
            "Gradient input: [[0.19661193 0.19661193 0.19661193 0.19661193 0.19661193]]\n",
            "‚úÖ Sigmoid —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n"
          ]
        }
      ],
      "source": [
        "# –¢–µ—Å—Ç Sigmoid (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Sigmoid)\n",
        "# print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Sigmoid –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "\n",
        "sigmoid = Sigmoid()\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "x_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.float32)\n",
        "\n",
        "# Forward pass\n",
        "output = sigmoid.forward(x_test)\n",
        "print(f\"Input: {x_test}\")\n",
        "print(f\"Output: {output}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—ã—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (0, 1)\n",
        "assert np.all(output > 0) and np.all(output < 1), \"Sigmoid –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (0, 1)\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ—Å—Ç—å: sigmoid(-x) = 1 - sigmoid(x)\n",
        "x_sym = np.array([[1]], dtype=np.float32)\n",
        "out_pos = sigmoid.forward(x_sym)\n",
        "out_neg = sigmoid.forward(-x_sym)\n",
        "assert np.allclose(out_neg, 1 - out_pos, atol=1e-6), \"Sigmoid –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º\"\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.ones_like(output)\n",
        "grad_input = sigmoid.backward(grad_output)\n",
        "print(f\"Gradient input: {grad_input}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π (sigmoid –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç)\n",
        "assert np.all(grad_input >= 0), \"–ì—Ä–∞–¥–∏–µ–Ω—Ç Sigmoid –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º\"\n",
        "\n",
        "print(\"‚úÖ Sigmoid —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDBOD5ztNw1B"
      },
      "source": [
        "## 3. –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Tanh\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**Tanh (–≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–π —Ç–∞–Ω–≥–µ–Ω—Å)** - —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è \"—Å–∂–∏–º–∞–µ—Ç\" –≤—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω (-1, 1).\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "- Forward: `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n",
        "- Backward: `df/dx = 1 - f(x)¬≤`\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–∞–¥ Sigmoid:**\n",
        "- –í—ã—Ö–æ–¥ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω –≤–æ–∫—Ä—É–≥ –Ω—É–ª—è\n",
        "- –ë–æ–ª—å—à–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "- –°–≤—è–∑—å —Å sigmoid: `tanh(x) = 2*sigmoid(2x) - 1`\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9ExyvVq8Nw1B"
      },
      "outputs": [],
      "source": [
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è Tanh\n",
        "\n",
        "        Args:\n",
        "            x: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä\n",
        "\n",
        "        Returns:\n",
        "            –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã, –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (-1, 1)\n",
        "        \"\"\"\n",
        "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ tanh —Ñ—É–Ω–∫—Ü–∏—é\n",
        "        self.output = np.tanh(x)\n",
        "        self.output = np.clip(self.output, -0.999999, 0.999999)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è Tanh\n",
        "\n",
        "        Args:\n",
        "            grad_output: –≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        Returns:\n",
        "            –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è\n",
        "        \"\"\"\n",
        "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç tanh\n",
        "\n",
        "        tanh_derivative = 1 - self.output**2\n",
        "        grad_input = tanh_derivative * grad_output\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKX-bVwyNw1C"
      },
      "source": [
        "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Tanh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujjVvkwHNw1C",
        "outputId": "8aa97958-e49a-4d16-b313-95026159a504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [[-10.  -1.   0.   1.  10.]]\n",
            "Output: [[-0.999999  -0.7615942  0.         0.7615942  0.999999 ]]\n",
            "Gradient input: [[1. 1. 1. 1. 1.]]\n",
            "‚úÖ Tanh —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n"
          ]
        }
      ],
      "source": [
        "# –¢–µ—Å—Ç Tanh (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Tanh)\n",
        "# print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Tanh –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "\n",
        "tanh = Tanh()\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "x_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.float32)\n",
        "\n",
        "# Forward pass\n",
        "output = tanh.forward(x_test)\n",
        "print(f\"Input: {x_test}\")\n",
        "print(f\"Output: {output}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—ã—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (-1, 1)\n",
        "assert np.all(output > -1) and np.all(output < 1), \"Tanh –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (-1, 1)\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º –∞–Ω—Ç–∏—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ—Å—Ç—å: tanh(-x) = -tanh(x)\n",
        "x_antisym = np.array([[2]], dtype=np.float32)\n",
        "out_pos = tanh.forward(x_antisym)\n",
        "out_neg = tanh.forward(-x_antisym)\n",
        "assert np.allclose(out_neg, -out_pos, atol=1e-6), \"Tanh –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∞–Ω—Ç–∏—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ tanh(0) = 0\n",
        "zero_out = tanh.forward(np.array([[0]], dtype=np.float32))\n",
        "assert np.allclose(zero_out, 0, atol=1e-6), \"tanh(0) –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–≤–µ–Ω 0\"\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.ones_like(output)\n",
        "grad_input = tanh.backward(grad_output)\n",
        "print(f\"Gradient input: {grad_input}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π (tanh –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç)\n",
        "assert np.all(grad_input >= 0), \"–ì—Ä–∞–¥–∏–µ–Ω—Ç Tanh –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º\"\n",
        "\n",
        "print(\"‚úÖ Tanh —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggxGupOQNw1C"
      },
      "source": [
        "## 4. –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (Linear/Dense)\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**–õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π** - –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π –±–ª–æ–∫ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π –∞—Ñ—Ñ–∏–Ω–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "- Forward: `y = x @ W + b`\n",
        "- –≥–¥–µ W - –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ —Ä–∞–∑–º–µ—Ä–∞ (input_size, output_size)\n",
        "- b - –≤–µ–∫—Ç–æ—Ä —Å–º–µ—â–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞ (output_size,)\n",
        "\n",
        "**–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã:**\n",
        "- `‚àÇL/‚àÇx = grad_output @ W.T`\n",
        "- `‚àÇL/‚àÇW = x.T @ grad_output`\n",
        "- `‚àÇL/‚àÇb = sum(grad_output, axis=0)`\n",
        "\n",
        "**–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤:**\n",
        "- Xavier/Glorot: –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–π\n",
        "- He: –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏–π\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rnNouHNtNw1C"
      },
      "outputs": [],
      "source": [
        "class Linear(Layer):\n",
        "    def __init__(self, input_size, output_size, bias=True):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.use_bias = bias\n",
        "\n",
        "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –≤–µ—Å–∞\n",
        "        std = np.sqrt(2.0 / input_size)\n",
        "        self.weight = np.random.randn(input_size,output_size) * std #method kaiming\n",
        "\n",
        "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ bias (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)\n",
        "        if self.use_bias:\n",
        "            self.bias = np.zeros(output_size)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "        self.input = None\n",
        "        self.grad_weight = np.zeros_like(self.weight)\n",
        "        if self.use_bias:\n",
        "            self.grad_bias = np.zeros_like(self.bias)\n",
        "        else:\n",
        "            self.grad_bias = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        Args:\n",
        "            x: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è backward pass\n",
        "        self.input = x.copy()\n",
        "\n",
        "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\n",
        "        output = x @ self.weight\n",
        "\n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        Args:\n",
        "            grad_output: –≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è —Ñ–æ—Ä–º—ã (batch_size, output_size)\n",
        "\n",
        "        Returns:\n",
        "            –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è —Ñ–æ—Ä–º—ã (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—Ö–æ–¥—É\n",
        "        grad_input = grad_output @ self.weight.T\n",
        "\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤–µ—Å–∞–º\n",
        "        self.grad_weight += self.input.T @ grad_output\n",
        "\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ bias\n",
        "        if self.use_bias:\n",
        "            self.grad_bias += np.sum(grad_output, axis=0)\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "    def update_weights(self, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞\n",
        "        \"\"\"\n",
        "        if self.grad_weight is not None:\n",
        "            self.weight -= learning_rate * self.grad_weight\n",
        "\n",
        "        if self.use_bias and self.grad_bias is not None:\n",
        "            self.bias -= learning_rate * self.grad_bias\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "        \"\"\"\n",
        "        self.grad_weight = np.zeros_like(self.grad_weight)\n",
        "        if self.use_bias:\n",
        "            self.grad_bias = np.zeros_like(self.grad_bias)\n",
        "\n",
        "    def parameters(self):\n",
        "      if self.bias is not None:\n",
        "        return (self.weight, self.bias)\n",
        "      else:\n",
        "        return(self.weight,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIokQDGPNw1C"
      },
      "source": [
        "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Linear\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2n-G_3qPNw1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fbff26-4b4b-43f5-a52b-7dc7fe1d8984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í–µ—Å–∞: \n",
            "[[ 0.40556541 -0.11289233]\n",
            " [ 0.52883548  1.24354867]\n",
            " [-0.19118543 -0.19117202]]\n",
            "Bias: [0. 0.]\n",
            "Input shape: (4, 3)\n",
            "Output shape: (4, 2)\n",
            "Expected shape: (4, 2)\n",
            "Gradient input shape: (4, 3)\n",
            "Gradient weight shape: (3, 2)\n",
            "Gradient bias shape: (2,)\n",
            "‚úÖ Linear —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n"
          ]
        }
      ],
      "source": [
        "# –¢–µ—Å—Ç Linear (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Linear)\n",
        "# print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Linear –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "\n",
        "linear = Linear(input_size=3, output_size=2, bias=True)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—É –≤–µ—Å–æ–≤\n",
        "assert linear.weight.shape == (3, 2), f\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≤–µ—Å–æ–≤: {linear.weight.shape}\"\n",
        "assert linear.bias.shape == (2,), f\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ bias: {linear.bias.shape}\"\n",
        "\n",
        "print(f\"–í–µ—Å–∞: \\n{linear.weight}\")\n",
        "print(f\"Bias: {linear.bias}\")\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "batch_size = 4\n",
        "x_test = np.random.randn(batch_size, 3).astype(np.float32)\n",
        "\n",
        "# Forward pass\n",
        "output = linear.forward(x_test)\n",
        "expected_shape = (batch_size, 2)\n",
        "\n",
        "print(f\"Input shape: {x_test.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Expected shape: {expected_shape}\")\n",
        "\n",
        "assert output.shape == expected_shape, f\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–∞: {output.shape}\"\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.random.randn(*output.shape).astype(np.float32)\n",
        "grad_input = linear.backward(grad_output)\n",
        "\n",
        "print(f\"Gradient input shape: {grad_input.shape}\")\n",
        "print(f\"Gradient weight shape: {linear.grad_weight.shape}\")\n",
        "print(f\"Gradient bias shape: {linear.grad_bias.shape}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "assert grad_input.shape == x_test.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ –≤—Ö–æ–¥—É\"\n",
        "assert linear.grad_weight.shape == linear.weight.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ –≤–µ—Å–∞–º\"\n",
        "assert linear.grad_bias.shape == linear.bias.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ bias\"\n",
        "\n",
        "print(\"‚úÖ Linear —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXtyfRLPNw1D"
      },
      "source": [
        "## 5. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (Sequential)\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**Sequential** - –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–µ–≤.\n",
        "\n",
        "**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**\n",
        "- Forward: –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å–ª–æ–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É: `output = layer_n(...layer_2(layer_1(input))...)`\n",
        "- Backward: –ø—Ä–∏–º–µ–Ω—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
        "\n",
        "**–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:**\n",
        "- –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö feed-forward —Å–µ—Ç–µ–π\n",
        "- –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–ª–æ–µ–≤ –≤ –±–ª–æ–∫–∏\n",
        "- –£–ø—Ä–æ—â–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∫–æ–¥–∞\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Z49Atm2dNw1D"
      },
      "outputs": [],
      "source": [
        "class Sequential(Layer):\n",
        "    def __init__(self, *layers):\n",
        "        super().__init__()\n",
        "        self.layers = list(layers)\n",
        "        self.layer_outputs = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        \"\"\"\n",
        "        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
        "        \"\"\"\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏\n",
        "\n",
        "        Args:\n",
        "            x: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä\n",
        "\n",
        "        Returns:\n",
        "            –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤\n",
        "        \"\"\"\n",
        "        # TODO: –û—á–∏—Å—Ç–∏—Ç–µ —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤\n",
        "        self.layer_outputs = []\n",
        "        self.layer_outputs.append(x)\n",
        "        # TODO: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –≤—Å–µ —Å–ª–æ–∏\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Å–ª–æ–π –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "            output = layer.forward(output)\n",
        "            self.layer_outputs.append(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
        "\n",
        "        Args:\n",
        "            grad_output: –≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        Returns:\n",
        "            –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è\n",
        "        \"\"\"\n",
        "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ backward –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤ –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
        "        grad = grad_output\n",
        "        for layer in reversed(self.layers):\n",
        "            # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ backward –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ—è\n",
        "            grad = layer.backward(grad)\n",
        "\n",
        "        return grad\n",
        "\n",
        "\n",
        "    def update_weights(self, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤—Å–µ—Ö —Å–ª–æ–µ–≤\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'update_weights'):\n",
        "                layer.update_weights(learning_rate)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤—Å–µ—Ö —Å–ª–æ–µ–≤\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'zero_grad'):\n",
        "                layer.zero_grad()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ–µ–≤ –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è\n",
        "        \"\"\"\n",
        "        super().train()\n",
        "        for layer in self.layers:\n",
        "            layer.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"\n",
        "        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ–µ–≤ –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
        "        \"\"\"\n",
        "        super().eval()\n",
        "        for layer in self.layers:\n",
        "            layer.eval()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.layers)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.layers[idx]\n",
        "\n",
        "    def parameters(self):\n",
        "       for layer in self.layers:\n",
        "        params = layer.parameters()\n",
        "        if isinstance (params, tuple):\n",
        "          yield from params\n",
        "        else:\n",
        "          yield params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtrnnTAjNw1D"
      },
      "source": [
        "## 6. Dropout\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**Dropout** - —Ç–µ—Ö–Ω–∏–∫–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–ª—É—á–∞–π–Ω–æ \"–≤—ã–∫–ª—é—á–∞–µ—Ç\" –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–π—Ä–æ–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.\n",
        "\n",
        "**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**\n",
        "- **–û–±—É—á–µ–Ω–∏–µ**: –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é `(1 - dropout_rate)`\n",
        "- **–ò–Ω—Ñ–µ—Ä–µ–Ω—Å**: –≤—Å–µ –Ω–µ–π—Ä–æ–Ω—ã –∞–∫—Ç–∏–≤–Ω—ã, –Ω–æ –≤—ã—Ö–æ–¥—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\n",
        "- –£–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å\n",
        "- –≠—Ñ—Ñ–µ–∫—Ç –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gLmKPCnMNw1D"
      },
      "outputs": [],
      "source": [
        "class Dropout(Layer):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è Dropout\n",
        "\n",
        "        Args:\n",
        "            x: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä\n",
        "\n",
        "        Returns:\n",
        "            –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º dropout (–≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.training:\n",
        "            # TODO: –°–æ–∑–¥–∞–π—Ç–µ –±–∏–Ω–∞—Ä–Ω—É—é –º–∞—Å–∫—É –¥–ª—è dropout\n",
        "            self.mask = (np.random.rand(*x.shape) > self.dropout_rate).astype(np.float32) / (1.0 - self.dropout_rate)\n",
        "\n",
        "            # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –º–∞—Å–∫—É –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "            output = x * self.mask\n",
        "        else:\n",
        "            # TODO: –í —Ä–µ–∂–∏–º–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
        "            output = x\n",
        "            self.mask = None\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è Dropout\n",
        "\n",
        "        Args:\n",
        "            grad_output: –≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "        Returns:\n",
        "            –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ç—É –∂–µ –º–∞—Å–∫—É –∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç—É\n",
        "            grad_input = grad_output * self.mask\n",
        "        else:\n",
        "            grad_input = grad_output\n",
        "\n",
        "        return grad_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEsMw9rcNw1D"
      },
      "source": [
        "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "T5sAWmYjNw1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc14d689-9550-444c-8784-39c3e306ca6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è:\n",
            "Input mean: 1.000\n",
            "Output mean: 1.008\n",
            "Proportion of zeros: 0.496\n",
            "\n",
            "–†–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:\n",
            "Output mean: 1.000\n",
            "Proportion of zeros: 0.000\n",
            "\n",
            "Gradient test:\n",
            "Grad input shape: (100, 10)\n",
            "Grad input mean: 1.018\n",
            "‚úÖ Dropout —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n"
          ]
        }
      ],
      "source": [
        "# –¢–µ—Å—Ç Dropout (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Dropout)\n",
        "# print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Dropout –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "\n",
        "dropout = Dropout(dropout_rate=0.5)\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "x_test = np.ones((100, 10), dtype=np.float32)\n",
        "\n",
        "# –¢–µ—Å—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è\n",
        "dropout.train()\n",
        "output_train = dropout.forward(x_test)\n",
        "\n",
        "print(f\"–†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è:\")\n",
        "print(f\"Input mean: {x_test.mean():.3f}\")\n",
        "print(f\"Output mean: {output_train.mean():.3f}\")\n",
        "print(f\"Proportion of zeros: {(output_train == 0).mean():.3f}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–æ–≤ \"–≤—ã–∫–ª—é—á–µ–Ω–∞\"\n",
        "zeros_ratio = (output_train == 0).mean()\n",
        "expected_zeros = 0.5  # dropout_rate\n",
        "assert abs(zeros_ratio - expected_zeros) < 0.1, f\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –¥–æ–ª—è –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {zeros_ratio}\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "assert abs(output_train.mean() - x_test.mean()) < 0.1, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è\"\n",
        "\n",
        "# –¢–µ—Å—Ç –≤ —Ä–µ–∂–∏–º–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
        "dropout.eval()\n",
        "output_eval = dropout.forward(x_test)\n",
        "\n",
        "print(f\"\\n–†–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:\")\n",
        "print(f\"Output mean: {output_eval.mean():.3f}\")\n",
        "print(f\"Proportion of zeros: {(output_eval == 0).mean():.3f}\")\n",
        "\n",
        "# –í —Ä–µ–∂–∏–º–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞—Ç—å—Å—è\n",
        "assert np.allclose(output_eval, x_test), \"–í —Ä–µ–∂–∏–º–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤—ã—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –≤—Ö–æ–¥–æ–º\"\n",
        "\n",
        "# –¢–µ—Å—Ç backward pass\n",
        "dropout.train()\n",
        "output_train = dropout.forward(x_test)\n",
        "grad_output = np.ones_like(output_train)\n",
        "grad_input = dropout.backward(grad_output)\n",
        "\n",
        "print(f\"\\nGradient test:\")\n",
        "print(f\"Grad input shape: {grad_input.shape}\")\n",
        "print(f\"Grad input mean: {grad_input.mean():.3f}\")\n",
        "\n",
        "assert grad_input.shape == x_test.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\"\n",
        "\n",
        "print(\"‚úÖ Dropout —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n6DHudTNw1D"
      },
      "source": [
        "## 7. Batch Normalization\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**Batch Normalization** - —Ç–µ—Ö–Ω–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π.\n",
        "\n",
        "**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**\n",
        "1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: `(x - mean) / sqrt(var + eps)`\n",
        "2. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–¥–≤–∏–≥: `gamma * normalized + beta`\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- –£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
        "- –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ learning rate\n",
        "- –£–º–µ–Ω—å—à–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "- –≠—Ñ—Ñ–µ–∫—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
        "\n",
        "**–†–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏:**\n",
        "- **–û–±—É—á–µ–Ω–∏–µ**: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ batch\n",
        "- **–ò–Ω—Ñ–µ—Ä–µ–Ω—Å**: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XNk3AI6iNw1D"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(Layer):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã gamma –∏ beta\n",
        "        self.gamma = np.ones(num_features)\n",
        "        self.beta = np.zeros(num_features)\n",
        "\n",
        "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "        self.running_mean = np.zeros(num_features)\n",
        "        self.running_var = np.ones(num_features)\n",
        "\n",
        "        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è backward pass\n",
        "        self.batch_mean = None\n",
        "        self.batch_var = None\n",
        "        self.normalized = None\n",
        "        self.input = None\n",
        "        self.grad_gamma = np.zeros(num_features)\n",
        "        self.grad_beta = np.zeros(num_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è Batch Normalization\n",
        "\n",
        "        Args:\n",
        "            x: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, num_features)\n",
        "\n",
        "        Returns:\n",
        "            –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã\n",
        "        \"\"\"\n",
        "        self.input = x\n",
        "\n",
        "        if self.training:\n",
        "            # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–∫—É—â–µ–≥–æ batch\n",
        "            self.batch_mean = np.mean(x, axis = 0 )\n",
        "            self.batch_var = np.var(x, axis = 0)\n",
        "\n",
        "            # TODO: –û–±–Ω–æ–≤–∏—Ç–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * self.batch_mean\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * self.batch_var\n",
        "\n",
        "            mean = self.batch_mean\n",
        "            var = self.batch_var\n",
        "        else:\n",
        "            # TODO: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        # TODO: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "        self.normalized = (x - mean) / np.sqrt(var + self.eps)\n",
        "\n",
        "        # TODO: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–¥–≤–∏–≥\n",
        "        output = self.gamma * self.normalized + self.beta\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è Batch Normalization\n",
        "        \"\"\"\n",
        "        n = self.input.shape[0]\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º\n",
        "        self.grad_gamma = np.sum(grad_output * self.normalized, axis =0)\n",
        "        self.grad_beta = np.sum(grad_output, axis =0)\n",
        "\n",
        "        grad_normalized = grad_output * self.gamma\n",
        "\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—Ö–æ–¥—É\n",
        "        grad_var = np.sum(grad_normalized * (self.input - self.batch_mean), axis=0) * (-0.5) * (self.batch_var + self.eps)**(-1.5)\n",
        "\n",
        "        grad_mean = np.sum(grad_normalized , axis=0) * (-1) / np.sqrt(self.batch_var + self.eps)\n",
        "        + grad_var * np.mean(-2 * (self.input - self.batch_mean), axis=0)\n",
        "\n",
        "        grad_input = (grad_normalized / np.sqrt(self.batch_var + self.eps)) + grad_var * 2 * (self.input - self.batch_mean) / n + grad_mean /n\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "    def update_weights(self, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "        \"\"\"\n",
        "        if self.grad_gamma is not None:\n",
        "            self.gamma -= learning_rate * self.grad_gamma\n",
        "\n",
        "        if self.grad_beta is not None:\n",
        "            self.beta -= learning_rate * self.grad_beta\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "        \"\"\"\n",
        "        self.grad_gamma = np.zeros(self.num_features)\n",
        "        self.grad_beta = np.zeros(self.num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "mtVFZ9waNw1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a2bfc7-d94a-439b-fd1e-3c97ab20dddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gamma: [1. 1. 1. 1.]\n",
            "Beta: [0. 0. 0. 0.]\n",
            "Input: \n",
            "[[1. 2. 3. 4.]\n",
            " [2. 3. 4. 5.]\n",
            " [3. 4. 5. 6.]]\n",
            "Input mean per feature: [2. 3. 4. 5.]\n",
            "Input std per feature: [0.8164966 0.8164966 0.8164966 0.8164966]\n",
            "\n",
            "Output: \n",
            "[[-1.22473562 -1.22473562 -1.22473562 -1.22473562]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 1.22473562  1.22473562  1.22473562  1.22473562]]\n",
            "Output mean per feature: [0. 0. 0. 0.]\n",
            "Output std per feature: [0.99999244 0.99999244 0.99999244 0.99999244]\n",
            "\n",
            "Running mean: [1.79999995 2.69999981 3.5999999  4.5       ]\n",
            "Running var: [0.70000002 0.70000002 0.70000002 0.70000002]\n",
            "\n",
            "Gradient input shape: (3, 4)\n",
            "Gradient gamma shape: (4,)\n",
            "Gradient beta shape: (4,)\n",
            "\n",
            "Inference mode output mean: [0.23904407 0.35856624 0.47808813 0.59761003]\n",
            "‚úÖ BatchNorm —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n"
          ]
        }
      ],
      "source": [
        "# –¢–µ—Å—Ç BatchNorm (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ BatchNorm)\n",
        "# print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ BatchNorm –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "\n",
        "batch_norm = BatchNorm(num_features=4)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é\n",
        "assert np.allclose(batch_norm.gamma, 1.0), \"Gamma –¥–æ–ª–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –µ–¥–∏–Ω–∏—Ü–∞–º–∏\"\n",
        "assert np.allclose(batch_norm.beta, 0.0), \"Beta –¥–æ–ª–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω—É–ª—è–º–∏\"\n",
        "assert np.allclose(batch_norm.running_mean, 0.0), \"Running mean –¥–æ–ª–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω—É–ª—è–º–∏\"\n",
        "assert np.allclose(batch_norm.running_var, 1.0), \"Running var –¥–æ–ª–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –µ–¥–∏–Ω–∏—Ü–∞–º–∏\"\n",
        "\n",
        "print(f\"Gamma: {batch_norm.gamma}\")\n",
        "print(f\"Beta: {batch_norm.beta}\")\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\n",
        "x_test = np.array([\n",
        "    [1, 2, 3, 4],\n",
        "    [2, 3, 4, 5],\n",
        "    [3, 4, 5, 6]\n",
        "], dtype=np.float32)\n",
        "\n",
        "print(f\"Input: \\n{x_test}\")\n",
        "print(f\"Input mean per feature: {x_test.mean(axis=0)}\")\n",
        "print(f\"Input std per feature: {x_test.std(axis=0)}\")\n",
        "\n",
        "# Forward pass –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è\n",
        "batch_norm.train()\n",
        "output = batch_norm.forward(x_test)\n",
        "\n",
        "print(f\"\\nOutput: \\n{output}\")\n",
        "print(f\"Output mean per feature: {output.mean(axis=0)}\")\n",
        "print(f\"Output std per feature: {output.std(axis=0)}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—ã—Ö–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω (—Å—Ä–µ–¥–Ω–µ–µ ‚âà 0, std ‚âà 1)\n",
        "assert np.allclose(output.mean(axis=0), 0, atol=1e-6), \"–°—Ä–µ–¥–Ω–µ–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–ª–∏–∑–∫–æ –∫ 0\"\n",
        "assert np.allclose(output.std(axis=0), 1, atol=1e-6), \"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–ª–∏–∑–∫–æ –∫ 1\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ running —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
        "print(f\"\\nRunning mean: {batch_norm.running_mean}\")\n",
        "print(f\"Running var: {batch_norm.running_var}\")\n",
        "\n",
        "# Backward pass\n",
        "grad_output = np.ones_like(output)\n",
        "grad_input = batch_norm.backward(grad_output)\n",
        "\n",
        "print(f\"\\nGradient input shape: {grad_input.shape}\")\n",
        "print(f\"Gradient gamma shape: {batch_norm.grad_gamma.shape}\")\n",
        "print(f\"Gradient beta shape: {batch_norm.grad_beta.shape}\")\n",
        "\n",
        "assert grad_input.shape == x_test.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ –≤—Ö–æ–¥—É\"\n",
        "assert batch_norm.grad_gamma.shape == batch_norm.gamma.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ gamma\"\n",
        "assert batch_norm.grad_beta.shape == batch_norm.beta.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ beta\"\n",
        "\n",
        "# –¢–µ—Å—Ç —Ä–µ–∂–∏–º–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
        "batch_norm.eval()\n",
        "output_eval = batch_norm.forward(x_test)\n",
        "print(f\"\\nInference mode output mean: {output_eval.mean(axis=0)}\")\n",
        "\n",
        "print(\"‚úÖ BatchNorm —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zr9F4b7Nw1E"
      },
      "source": [
        "## 8. Adam –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**Adam (Adaptive Moment Estimation)** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, —Å–æ—á–µ—Ç–∞—é—â–∏–π –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ RMSprop –∏ Momentum.\n",
        "\n",
        "**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**\n",
        "1. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (momentum)\n",
        "2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (RMSprop)\n",
        "3. –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è (bias correction)\n",
        "4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ learning rate –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
        "- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ\n",
        "- –¢—Ä–µ–±—É–µ—Ç –º–∞–ª–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "-W82cQkUNw1E"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "\n",
        "        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è\n",
        "        self.m = {}  # first moment\n",
        "        self.v = {}  # second moment\n",
        "        self.t = 0   # time step\n",
        "\n",
        "    def update(self, layer, layer_id):\n",
        "        \"\"\"\n",
        "        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ª–æ—è —Å –ø–æ–º–æ—â—å—é Adam\n",
        "\n",
        "        Args:\n",
        "            layer: —Å–ª–æ–π —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏\n",
        "            layer_id: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ª–æ—è\n",
        "        \"\"\"\n",
        "        if not hasattr(self, '_layer_steps'):
            self._layer_steps = {}
        
        if layer_id not in self._layer_steps:
            self._layer_steps[layer_id] = 0
        
        self._layer_steps[layer_id] += 1
        t = self._layer_steps[layer_id]\n",
        "\n",
        "        # TODO: –û–±–Ω–æ–≤–∏—Ç–µ –≤–µ—Å–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
        "        if hasattr(layer, 'grad_weight') and layer.grad_weight is not None:\n",
        "            grad_w = layer.grad_weight\n",
        "            # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –º–æ–º–µ–Ω—Ç—ã –¥–ª—è –≤–µ—Å–æ–≤\n",
        "            if f\"{layer_id}_weight\" not in self.m:\n",
        "                self.m[f\"{layer_id}_weight\"] = np.zeros_like(layer.weight)\n",
        "                self.v[f\"{layer_id}_weight\"] = np.zeros_like(layer.weight)\n",
        "\n",
        "            # TODO: –û–±–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç (momentum)\n",
        "            self.m[f\"{layer_id}_weight\"] = (self.beta1 * self.m[f\"{layer_id}_weight\"] + (1 - self.beta1) * grad_w)\n",
        "\n",
        "            # TODO: –û–±–Ω–æ–≤–∏—Ç–µ –≤—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç (RMSprop)\n",
        "            self.v[f\"{layer_id}_weight\"] = (self.beta2 * self.v[f\"{layer_id}_weight\"] + (1 - self.beta2) * (grad_w ** 2))\n",
        "\n",
        "            # TODO: –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è\n",
        "            m_corrected = self.m[f\"{layer_id}_weight\"] / (1 - self.beta1 ** self.t)\n",
        "            v_corrected = self.v[f\"{layer_id}_weight\"] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            # TODO: –û–±–Ω–æ–≤–∏—Ç–µ –≤–µ—Å–∞\n",
        "            layer.weight -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.eps)\n",
        "\n",
        "        # TODO: –û–±–Ω–æ–≤–∏—Ç–µ bias –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –≤–µ—Å–∞–º\n",
        "        if hasattr(layer, 'grad_bias') and layer.grad_bias is not None:\n",
        "          grad_b = layer.grad_bias\n",
        "          if f\"{layer_id}_bias\" not in self.m:\n",
        "              self.m[f\"{layer_id}_bias\"] = np.zeros_like(layer.bias)\n",
        "              self.v[f\"{layer_id}_bias\"] = np.zeros_like(layer.bias)\n",
        "\n",
        "          # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ bias\n",
        "          # –û–±–Ω–æ–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç (momentum)\n",
        "          self.m[f\"{layer_id}_bias\"] = (self.beta1 * self.m[f\"{layer_id}_bias\"] +\n",
        "                                        (1 - self.beta1) * grad_b)\n",
        "          # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç (RMSprop)\n",
        "          self.v[f\"{layer_id}_bias\"] = (self.beta2 * self.v[f\"{layer_id}_bias\"] +\n",
        "                                        (1 - self.beta2) * (grad_b ** 2))\n",
        "\n",
        "          # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è\n",
        "          m_corrected_b = self.m[f\"{layer_id}_bias\"] / (1 - self.beta1 ** self.t)\n",
        "          v_corrected_b = self.v[f\"{layer_id}_bias\"] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "          layer.bias -= self.learning_rate * m_corrected_b / (np.sqrt(v_corrected_b) + self.eps)\n",
        "\n",
        "    def zero_grad(self, layers):\n",
        "        \"\"\"\n",
        "        –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "        \"\"\"\n",
        "        for layer in layers:\n",
        "            if hasattr(layer, 'grad_weight'):\n",
        "                layer.grad_weight = None\n",
        "            if hasattr(layer, 'grad_bias'):\n",
        "                layer.grad_bias = None\n",
        "            if hasattr(layer, 'grad_gamma'):\n",
        "                layer.grad_gamma = None\n",
        "            if hasattr(layer, 'grad_beta'):\n",
        "                layer.grad_beta = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XMNmrthKNw1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e79158d-3943-45a7-bd04-7cab83d73dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weight: \n",
            "[[-0.25727419  1.13583   ]\n",
            " [-1.18942608 -0.06203191]\n",
            " [ 1.23175618  0.12678166]]\n",
            "Initial bias: [0. 0.]\n",
            "Weight gradient: \n",
            "[[0.1 0.2]\n",
            " [0.3 0.4]\n",
            " [0.5 0.6]]\n",
            "Bias gradient: [0.1 0.2]\n",
            "\n",
            "After 1 step:\n",
            "Updated weight: \n",
            "[[-0.26727419  1.12583   ]\n",
            " [-1.19942608 -0.07203191]\n",
            " [ 1.22175618  0.11678166]]\n",
            "Updated bias: [-0.01 -0.01]\n",
            "Time step: 1\n",
            "‚úÖ Adam —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n"
          ]
        }
      ],
      "source": [
        "# –¢–µ—Å—Ç Adam (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Adam)\n",
        "# print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Adam –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è\n",
        "layer = Linear(3, 2)\n",
        "adam = Adam(learning_rate=0.01)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "layer.grad_weight = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype=np.float32)\n",
        "layer.grad_bias = np.array([0.1, 0.2], dtype=np.float32)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤\n",
        "initial_weight = layer.weight.copy()\n",
        "initial_bias = layer.bias.copy()\n",
        "\n",
        "print(f\"Initial weight: \\n{initial_weight}\")\n",
        "print(f\"Initial bias: {initial_bias}\")\n",
        "print(f\"Weight gradient: \\n{layer.grad_weight}\")\n",
        "print(f\"Bias gradient: {layer.grad_bias}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é Adam\n",
        "assert len(adam.m) == 0, \"–ú–æ–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\"\n",
        "assert len(adam.v) == 0, \"–ú–æ–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\"\n",
        "assert adam.t == 0, \"Time step –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–≤–µ–Ω 0\"\n",
        "\n",
        "# –í—ã–ø–æ–ª–Ω–∏–º –æ–¥–∏–Ω —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
        "adam.update(layer, \"test_layer\")\n",
        "\n",
        "print(f\"\\nAfter 1 step:\")\n",
        "print(f\"Updated weight: \\n{layer.weight}\")\n",
        "print(f\"Updated bias: {layer.bias}\")\n",
        "print(f\"Time step: {adam.t}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤–µ—Å–∞ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å\n",
        "assert not np.allclose(layer.weight, initial_weight), \"–í–µ—Å–∞ –¥–æ–ª–∂–Ω—ã –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\"\n",
        "assert not np.allclose(layer.bias, initial_bias), \"Bias –¥–æ–ª–∂–µ–Ω –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –º–æ–º–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã\n",
        "assert \"test_layer_weight\" in adam.m, \"–ú–æ–º–µ–Ω—Ç –¥–ª—è –≤–µ—Å–æ–≤ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω\"\n",
        "assert \"test_layer_bias\" in adam.m, \"–ú–æ–º–µ–Ω—Ç –¥–ª—è bias –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω\"\n",
        "assert \"test_layer_weight\" in adam.v, \"–ú–æ–º–µ–Ω—Ç –¥–ª—è –≤–µ—Å–æ–≤ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω\"\n",
        "assert \"test_layer_bias\" in adam.v, \"–ú–æ–º–µ–Ω—Ç –¥–ª—è bias –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—ã –º–æ–º–µ–Ω—Ç–æ–≤\n",
        "assert adam.m[\"test_layer_weight\"].shape == layer.weight.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –º–æ–º–µ–Ω—Ç–∞ –≤–µ—Å–æ–≤\"\n",
        "assert adam.m[\"test_layer_bias\"].shape == layer.bias.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –º–æ–º–µ–Ω—Ç–∞ bias\"\n",
        "\n",
        "# –¢–µ—Å—Ç zero_grad\n",
        "adam.zero_grad([layer])\n",
        "assert layer.grad_weight is None, \"–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–µ—Å–æ–≤ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–Ω—É–ª–µ–Ω—ã\"\n",
        "assert layer.grad_bias is None, \"–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã bias –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–Ω—É–ª–µ–Ω—ã\"\n",
        "\n",
        "print(\"‚úÖ Adam —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT1q-eDlNw1E"
      },
      "source": [
        "## 9. –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
        "\n",
        "### –¢–µ–æ—Ä–∏—è\n",
        "\n",
        "**–§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å** –∏–∑–º–µ—Ä—è—é—Ç —Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.\n",
        "\n",
        "**Cross-Entropy Loss:**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "- –§–æ—Ä–º—É–ª–∞: `-log(p_correct_class)`\n",
        "- –®—Ç—Ä–∞—Ñ—É–µ—Ç –Ω–µ–≤–µ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ\n",
        "\n",
        "**Mean Squared Error (MSE):**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
        "- –§–æ—Ä–º—É–ª–∞: `(y_pred - y_true)¬≤`\n",
        "- –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º\n",
        "\n",
        "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "3q9GfngRNw1E"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss:\n",
        "    def __init__(self):\n",
        "        self.predictions = None\n",
        "        self.targets = None\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Cross-Entropy Loss\n",
        "\n",
        "        Args:\n",
        "            predictions: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (batch_size, num_classes)\n",
        "            targets: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∞ (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
        "        \"\"\"\n",
        "        self.predictions = predictions\n",
        "        self.targets = targets\n",
        "\n",
        "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ softmax –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n",
        "        self.softmax_pred = softmax(predictions)\n",
        "\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ cross-entropy loss\n",
        "        batch_size = predictions.shape[0]\n",
        "\n",
        "        correct_class_probs = self.softmax_pred[np.arange(batch_size), targets]\n",
        "        loss = -np.mean(np.log(correct_class_probs + 1e-15))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ Cross-Entropy Loss\n",
        "\n",
        "        Returns:\n",
        "            –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n",
        "        \"\"\"\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç\n",
        "        batch_size = self.predictions.shape[0]\n",
        "        num_classes = self.predictions.shape[1]\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º one-hot –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è —Ü–µ–ª–µ–π\n",
        "        one_hot_targets = one_hot_encode(self.targets, num_classes)\n",
        "\n",
        "        # –ì—Ä–∞–¥–∏–µ–Ω—Ç: (softmax_pred - one_hot_targets) / batch_size\n",
        "        grad = (self.softmax_pred - one_hot_targets) / batch_size\n",
        "\n",
        "        return grad\n",
        "\n",
        "\n",
        "class MSELoss:\n",
        "    def __init__(self):\n",
        "        self.predictions = None\n",
        "        self.targets = None\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Mean Squared Error\n",
        "\n",
        "        Args:\n",
        "            predictions: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
        "            targets: –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "\n",
        "        Returns:\n",
        "            –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
        "        \"\"\"\n",
        "        self.predictions = predictions\n",
        "        self.targets = targets\n",
        "\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ MSE\n",
        "        loss = np.mean((predictions - targets) ** 2)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ MSE\n",
        "\n",
        "        Returns:\n",
        "            –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n",
        "        \"\"\"\n",
        "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç MSE\n",
        "        batch_size = self.predictions.shape[0]\n",
        "        grad = 2 * (self.predictions - self.targets) / batch_size\n",
        "\n",
        "        return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    –£—Å—Ç–æ–π—á–∏–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è softmax\n",
        "    \"\"\"\n",
        "    # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ softmax —Ñ—É–Ω–∫—Ü–∏—é\n",
        "    x_stable = x - np.max(x, axis=1, keepdims=True)\n",
        "    exp_x = np.exp(x_stable)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ one-hot –∫–æ–¥–∏—Ä–æ–≤–∫—É\n",
        "    \"\"\"\n",
        "    # TODO: –°–æ–∑–¥–∞–π—Ç–µ one-hot –∫–æ–¥–∏—Ä–æ–≤–∫—É\n",
        "    batch_size = labels.shape[0]\n",
        "    one_hot = np.zeros((batch_size, num_classes))\n",
        "    one_hot[np.arange(batch_size), labels] = 1\n",
        "    return one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "EdWXYCbRNw1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35228e3f-48af-48f8-d802-a1b0a0ca4b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ CrossEntropyLoss...\n",
            "Predictions: \n",
            "[[2.  1.  0.1]\n",
            " [1.  3.  0.2]]\n",
            "Targets: [0 1]\n",
            "CrossEntropy Loss: 0.2981\n",
            "Gradient shape: (2, 3)\n",
            "Gradient: \n",
            "[[-0.17049944  0.12121648  0.04928295]\n",
            " [ 0.05657142 -0.08199063  0.02541918]]\n",
            "‚úÖ CrossEntropyLoss —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\n",
            "\n",
            "üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ MSELoss...\n",
            "Predictions: \n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "Targets: \n",
            "[[1.5 2.5]\n",
            " [2.5 3.5]]\n",
            "MSE Loss: 0.2500\n",
            "MSE Gradient shape: (2, 2)\n",
            "MSE Gradient: \n",
            "[[-0.5 -0.5]\n",
            " [ 0.5  0.5]]\n",
            "‚úÖ MSELoss —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\n",
            "\n",
            "üéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Softmax...\n",
            "Input: \n",
            "[[1. 2. 3.]\n",
            " [1. 1. 1.]]\n",
            "Softmax output: \n",
            "[[0.09003057 0.24472846 0.66524094]\n",
            " [0.33333334 0.33333334 0.33333334]]\n",
            "‚úÖ Softmax —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\n",
            "\n",
            "üè∑Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ One-hot encoding...\n",
            "Labels: [0 2 1 0]\n",
            "One-hot: \n",
            "[[1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n",
            "‚úÖ One-hot encoding —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\n",
            "\n",
            "üéâ –í—Å–µ —Ç–µ—Å—Ç—ã —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\n"
          ]
        }
      ],
      "source": [
        "# –¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Loss —Ñ—É–Ω–∫—Ü–∏–π)\n",
        "# print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "\n",
        "# –¢–µ—Å—Ç CrossEntropyLoss\n",
        "print(\"üî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ CrossEntropyLoss...\")\n",
        "ce_loss = CrossEntropyLoss()\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "predictions = np.array([[2.0, 1.0, 0.1], [1.0, 3.0, 0.2]], dtype=np.float32)\n",
        "targets = np.array([0, 1], dtype=np.int32)\n",
        "\n",
        "print(f\"Predictions: \\n{predictions}\")\n",
        "print(f\"Targets: {targets}\")\n",
        "\n",
        "# Forward pass\n",
        "loss_value = ce_loss.forward(predictions, targets)\n",
        "print(f\"CrossEntropy Loss: {loss_value:.4f}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ loss –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π\n",
        "assert loss_value > 0, \"CrossEntropy loss –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
        "\n",
        "# Backward pass\n",
        "grad = ce_loss.backward()\n",
        "print(f\"Gradient shape: {grad.shape}\")\n",
        "print(f\"Gradient: \\n{grad}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n",
        "assert grad.shape == predictions.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ CrossEntropy\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —Å—É–º–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º —Ä–∞–≤–Ω–∞ 0 (—Å–≤–æ–π—Å—Ç–≤–æ softmax)\n",
        "assert np.allclose(grad.sum(axis=1), 0, atol=1e-6), \"–°—É–º–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 0\"\n",
        "\n",
        "print(\"‚úÖ CrossEntropyLoss —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\")\n",
        "\n",
        "# –¢–µ—Å—Ç MSELoss\n",
        "print(\"\\nüìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ MSELoss...\")\n",
        "mse_loss = MSELoss()\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
        "predictions_reg = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n",
        "targets_reg = np.array([[1.5, 2.5], [2.5, 3.5]], dtype=np.float32)\n",
        "\n",
        "print(f\"Predictions: \\n{predictions_reg}\")\n",
        "print(f\"Targets: \\n{targets_reg}\")\n",
        "\n",
        "# Forward pass\n",
        "mse_value = mse_loss.forward(predictions_reg, targets_reg)\n",
        "print(f\"MSE Loss: {mse_value:.4f}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ loss –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π\n",
        "assert mse_value >= 0, \"MSE loss –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º\"\n",
        "\n",
        "# Backward pass\n",
        "grad_mse = mse_loss.backward()\n",
        "print(f\"MSE Gradient shape: {grad_mse.shape}\")\n",
        "print(f\"MSE Gradient: \\n{grad_mse}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n",
        "assert grad_mse.shape == predictions_reg.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ MSE\"\n",
        "\n",
        "print(\"‚úÖ MSELoss —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\")\n",
        "\n",
        "# –¢–µ—Å—Ç softmax —Ñ—É–Ω–∫—Ü–∏–∏\n",
        "print(\"\\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Softmax...\")\n",
        "x_softmax = np.array([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]], dtype=np.float32)\n",
        "softmax_output = softmax(x_softmax)\n",
        "\n",
        "print(f\"Input: \\n{x_softmax}\")\n",
        "print(f\"Softmax output: \\n{softmax_output}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —Å—É–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Ä–∞–≤–Ω–∞ 1\n",
        "assert np.allclose(softmax_output.sum(axis=1), 1.0), \"–°—É–º–º–∞ softmax –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–∞–≤–Ω–∞ 1\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ\n",
        "assert np.all(softmax_output > 0), \"–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è softmax –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏\"\n",
        "assert np.all(softmax_output < 1), \"–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è softmax –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –º–µ–Ω—å—à–µ 1\"\n",
        "\n",
        "print(\"‚úÖ Softmax —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\")\n",
        "\n",
        "# –¢–µ—Å—Ç one-hot encoding\n",
        "print(\"\\nüè∑Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ One-hot encoding...\")\n",
        "labels = np.array([0, 2, 1, 0])\n",
        "one_hot = one_hot_encode(labels, num_classes=3)\n",
        "\n",
        "print(f\"Labels: {labels}\")\n",
        "print(f\"One-hot: \\n{one_hot}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—É\n",
        "assert one_hot.shape == (4, 3), \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ one-hot –∫–æ–¥–∏—Ä–æ–≤–∫–∏\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–æ–≤–Ω–æ –æ–¥–Ω—É –µ–¥–∏–Ω–∏—Ü—É\n",
        "assert np.all(one_hot.sum(axis=1) == 1), \"–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ä–æ–≤–Ω–æ –æ–¥–Ω—É –µ–¥–∏–Ω–∏—Ü—É\"\n",
        "\n",
        "print(\"‚úÖ One-hot encoding —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\")\n",
        "\n",
        "print(\"\\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czv4F9nwNw1F"
      },
      "source": [
        "## 10. –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "\n",
        "–¢–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞–π—Ç–µ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, X, y, batch_size=32, shuffle=False, drop_last=False):\n",
        "        \"\"\"\n",
        "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "        Args:\n",
        "            dataset: list –∏–ª–∏ np.array, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ\n",
        "            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
        "            shuffle: –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.drop_last = drop_last\n",
        "\n",
        "        self._shuffle_data()\n",
        "\n",
        "    def _shuffle_data(self):\n",
        "        \"\"\"–ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "        self.array = list(range(len(self.dataset)))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.array)\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞\"\"\"\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞ (–≤–µ–∫—Ç–æ—Ä—ã + –º–µ—Ç–∫–∏)\"\"\"\n",
        "        if len(self.array) == 0:\n",
        "            self._shuffle_data()\n",
        "            raise StopIteration()\n",
        "\n",
        "        if len(self.array) < self.batch_size and self.drop_last:\n",
        "            self._shuffle_data()\n",
        "            raise StopIteration()\n",
        "\n",
        "        if len(self.array) > self.batch_size:\n",
        "            selected = self.array[:self.batch_size]\n",
        "            self.array = self.array[self.batch_size:]\n",
        "        else:\n",
        "            selected = self.array\n",
        "            self.array = []\n",
        "\n",
        "        data = [self.dataset[ind][0] for ind in selected]\n",
        "        labels = [self.dataset[ind][1] for ind in selected]\n",
        "        return np.array(data, dtype=np.float32), np.array(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\"\"\"\n",
        "        return int(np.ceil(len(self.dataset) / self.batch_size))\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\"\"\"\n",
        "        return self.dataset\n",
        ""
      ],
      "metadata": {
        "id": "jFPCj9vEiLQI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "f0hxt8lANw1F"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, batch_size=128, epochs=10):\n",
        "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
        "        self.model = Sequential(\n",
        "\n",
        "            Linear(784,512),\n",
        "            BatchNorm(512),\n",
        "            ReLU(),\n",
        "            Dropout(0.4),\n",
        "\n",
        "            Linear(512,256),\n",
        "            BatchNorm(256),\n",
        "            ReLU(),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Linear(256,128),\n",
        "            BatchNorm(128),\n",
        "            ReLU(),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Linear(128,10)\n",
        "        )\n",
        "        self.optimizer = Adam(self.model.parameters())\n",
        "        self.loss_func = CrossEntropyLoss\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model.forward(x)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        return self.model.backward(grad_output)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_trainable_layers(self):\n",
        "        \"\"\"\n",
        "        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ–µ–≤ —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "        \"\"\"\n",
        "        trainable_layers = []\n",
        "        for layer in self.model.layers:\n",
        "            if hasattr(layer, 'update_weights'):\n",
        "                trainable_layers.append(layer)\n",
        "        return trainable_layers\n",
        "\n",
        "    def compute_loss(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
        "        \"\"\"\n",
        "        return self.loss_func.forward(predictions, targets)\n",
        "\n",
        "    def compute_grad_loss(self):\n",
        "        \"\"\"\n",
        "        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
        "        \"\"\"\n",
        "        return self.loss_fn.backward()\n",
        "\n",
        "    def train_model(self, dataset):\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
        "      for epoch in range(self.epochs):\n",
        "        print(f\"Epoch: {epoch + 1} of {self.epochs}\")\n",
        "        for x_batch, y_batch in tqdm(dataloader):\n",
        "          self.optimizer.zero_grad()\n",
        "          predictions = self.model.forward(x_batch)\n",
        "          loss = self.loss_func(predictions, y_batch)\n",
        "          loss.backward()\n",
        "          self.optimizer.update()\n",
        "      return self\n",
        "\n",
        "    def predict_model(self, dataset):\n",
        "      dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False)\n",
        "      predictions = []\n",
        "      for x_batch, y_batch in dataloader:\n",
        "        result = self.model.forward(x_batch)\n",
        "        batch_predictions = np.argmax(result.array, axis=1)\n",
        "        predictions.extend(batch_predictions)\n",
        "      return np.array(predictions)\n",
        "\n",
        "\n",
        "# TODO: –°–æ–∑–¥–∞–π—Ç–µ —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ç–∏\n",
        "net = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "X = np.random.randn(num_samples, 784)  # 1000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π 28x28\n",
        "y = np.random.randint(0, 10, num_samples)  # 10 –∫–ª–∞—Å—Å–æ–≤\n",
        "dataset = (X, y)"
      ],
      "metadata": {
        "id": "mxYHNP9Gtiwb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ\n",
        "nn = NeuralNetwork(batch_size=128, epochs=20)\n",
        "nn.train_model(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MIsyybEruSsA",
        "outputId": "27f6ac70-5339-488e-a4e0-9c2297f28d54"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'tuple' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2063195112.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4086256223.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch + 1} of {self.epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2505752269.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, drop_last)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_shuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2505752269.py\u001b[0m in \u001b[0;36m_shuffle_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_shuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m\"\"\"–ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}