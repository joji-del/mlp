#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
"""

import math
import random

# –ó–∞–º–µ–Ω–∞ numpy - –ø—Ä–æ—Å—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
def random_normal(shape, mean=0, std=1):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º–∏ —á–∏—Å–ª–∞–º–∏ –∏–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
    if isinstance(shape, int):
        return [random.gauss(mean, std) for _ in range(shape)]
    elif len(shape) == 1:
        return [random.gauss(mean, std) for _ in range(shape[0])]
    elif len(shape) == 2:
        return [[random.gauss(mean, std) for _ in range(shape[1])] for _ in range(shape[0])]

def zeros(shape):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ –Ω—É–ª–µ–π"""
    if isinstance(shape, int):
        return [0.0 for _ in range(shape)]
    elif len(shape) == 1:
        return [0.0 for _ in range(shape[0])]
    elif len(shape) == 2:
        return [[0.0 for _ in range(shape[1])] for _ in range(shape[0])]

def ones(shape):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ –µ–¥–∏–Ω–∏—Ü"""
    if isinstance(shape, int):
        return [1.0 for _ in range(shape)]
    elif len(shape) == 1:
        return [1.0 for _ in range(shape[0])]
    elif len(shape) == 2:
        return [[1.0 for _ in range(shape[1])] for _ in range(shape[0])]

def matrix_multiply(A, B):
    """–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü"""
    rows_A, cols_A = len(A), len(A[0])
    rows_B, cols_B = len(B), len(B[0])
    
    if cols_A != rows_B:
        raise ValueError("–ù–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ä–∞–∑–º–µ—Ä—ã –º–∞—Ç—Ä–∏—Ü –¥–ª—è —É–º–Ω–æ–∂–µ–Ω–∏—è")
    
    result = zeros((rows_A, cols_B))
    for i in range(rows_A):
        for j in range(cols_B):
            for k in range(cols_A):
                result[i][j] += A[i][k] * B[k][j]
    
    return result

def matrix_transpose(A):
    """–¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã"""
    return [[A[j][i] for j in range(len(A))] for i in range(len(A[0]))]

def vector_add(a, b):
    """–°–ª–æ–∂–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤"""
    return [a[i] + b[i] for i in range(len(a))]

def matrix_add(A, B):
    """–°–ª–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü"""
    return [[A[i][j] + B[i][j] for j in range(len(A[0]))] for i in range(len(A))]

def apply_function(matrix, func):
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É"""
    if isinstance(matrix[0], list):
        return [[func(matrix[i][j]) for j in range(len(matrix[0]))] for i in range(len(matrix))]
    else:
        return [func(x) for x in matrix]

def mean(arr, axis=None):
    """–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ"""
    if axis is None:
        flat = [item for row in arr for item in (row if isinstance(row, list) else [row])]
        return sum(flat) / len(flat)
    elif axis == 0:
        return [sum(arr[i][j] for i in range(len(arr))) / len(arr) for j in range(len(arr[0]))]

def variance(arr, axis=None):
    """–î–∏—Å–ø–µ—Ä—Å–∏—è"""
    if axis == 0:
        means = mean(arr, axis=0)
        n = len(arr)
        return [sum((arr[i][j] - means[j])**2 for i in range(n)) / n for j in range(len(arr[0]))]

def sqrt(x):
    """–ö–≤–∞–¥—Ä–∞—Ç–Ω—ã–π –∫–æ—Ä–µ–Ω—å"""
    if isinstance(x, list):
        return [math.sqrt(val) for val in x]
    else:
        return math.sqrt(x)

# –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è
class SimpleLinear:
    def __init__(self, input_size, output_size):
        self.input_size = input_size
        self.output_size = output_size
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        std = math.sqrt(2.0 / input_size)
        self.weight = random_normal((input_size, output_size), 0, std)
        self.bias = zeros(output_size)
        
        self.input = None
        self.grad_weight = zeros((input_size, output_size))
        self.grad_bias = zeros(output_size)
    
    def forward(self, x):
        self.input = [row[:] for row in x]  # –∫–æ–ø–∏—è
        output = matrix_multiply(x, self.weight)
        
        # –î–æ–±–∞–≤–ª—è–µ–º bias
        for i in range(len(output)):
            output[i] = vector_add(output[i], self.bias)
        
        return output
    
    def backward(self, grad_output):
        # grad_input = grad_output @ weight.T
        weight_T = matrix_transpose(self.weight)
        grad_input = matrix_multiply(grad_output, weight_T)
        
        # grad_weight = input.T @ grad_output  
        input_T = matrix_transpose(self.input)
        self.grad_weight = matrix_multiply(input_T, grad_output)
        
        # grad_bias = sum(grad_output, axis=0)
        self.grad_bias = [sum(grad_output[i][j] for i in range(len(grad_output))) for j in range(len(grad_output[0]))]
        
        return grad_input

# –ü—Ä–æ—Å—Ç–æ–π ReLU
class SimpleReLU:
    def __init__(self):
        self.input = None
    
    def forward(self, x):
        self.input = [row[:] for row in x]  # –∫–æ–ø–∏—è
        return apply_function(x, lambda val: max(0, val))
    
    def backward(self, grad_output):
        result = zeros((len(grad_output), len(grad_output[0])))
        for i in range(len(grad_output)):
            for j in range(len(grad_output[0])):
                result[i][j] = grad_output[i][j] if self.input[i][j] > 0 else 0
        return result

# –ü—Ä–æ—Å—Ç–æ–π CrossEntropy Loss
def softmax(x):
    """–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è softmax"""
    result = []
    for row in x:
        # –î–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Ç–∞–µ–º –º–∞–∫—Å–∏–º—É–º
        max_val = max(row)
        exp_row = [math.exp(val - max_val) for val in row]
        sum_exp = sum(exp_row)
        result.append([val / sum_exp for val in exp_row])
    return result

def cross_entropy_loss(predictions, targets):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ cross-entropy loss"""
    batch_size = len(predictions)
    softmax_pred = softmax(predictions)
    
    total_loss = 0
    for i in range(batch_size):
        # –ò–∑–±–µ–≥–∞–µ–º log(0)
        prob = max(softmax_pred[i][targets[i]], 1e-15)
        total_loss += -math.log(prob)
    
    return total_loss / batch_size, softmax_pred

def cross_entropy_backward(softmax_pred, targets):
    """–ì—Ä–∞–¥–∏–µ–Ω—Ç cross-entropy loss"""
    batch_size = len(softmax_pred)
    num_classes = len(softmax_pred[0])
    
    grad = zeros((batch_size, num_classes))
    for i in range(batch_size):
        for j in range(num_classes):
            if j == targets[i]:
                grad[i][j] = (softmax_pred[i][j] - 1) / batch_size
            else:
                grad[i][j] = softmax_pred[i][j] / batch_size
    
    return grad

# –ü—Ä–æ—Å—Ç–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
class SimpleNetwork:
    def __init__(self):
        self.layer1 = SimpleLinear(4, 8)  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
        self.relu1 = SimpleReLU()
        self.layer2 = SimpleLinear(8, 3)  # 3 –∫–ª–∞—Å—Å–∞
        
        self.learning_rate = 0.01
    
    def forward(self, x):
        x = self.layer1.forward(x)
        x = self.relu1.forward(x)
        x = self.layer2.forward(x)
        return x
    
    def backward(self, grad_output):
        grad = self.layer2.backward(grad_output)
        grad = self.relu1.backward(grad)
        grad = self.layer1.backward(grad)
        return grad
    
    def update_weights(self):
        # –ü—Ä–æ—Å—Ç–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ (SGD)
        for i in range(len(self.layer1.weight)):
            for j in range(len(self.layer1.weight[0])):
                self.layer1.weight[i][j] -= self.learning_rate * self.layer1.grad_weight[i][j]
        
        for i in range(len(self.layer1.bias)):
            self.layer1.bias[i] -= self.learning_rate * self.layer1.grad_bias[i]
        
        for i in range(len(self.layer2.weight)):
            for j in range(len(self.layer2.weight[0])):
                self.layer2.weight[i][j] -= self.learning_rate * self.layer2.grad_weight[i][j]
        
        for i in range(len(self.layer2.bias)):
            self.layer2.bias[i] -= self.learning_rate * self.layer2.grad_bias[i]
    
    def zero_grad(self):
        self.layer1.grad_weight = zeros((self.layer1.input_size, self.layer1.output_size))
        self.layer1.grad_bias = zeros(self.layer1.output_size)
        self.layer2.grad_weight = zeros((self.layer2.input_size, self.layer2.output_size))
        self.layer2.grad_bias = zeros(self.layer2.output_size)

def create_simple_dataset():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    random.seed(42)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
    X = []
    y = []
    
    for _ in range(100):
        # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤
        if random.random() < 0.33:
            # –ö–ª–∞—Å—Å 0: –±–æ–ª—å—à–∏–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø–µ—Ä–≤—ã—Ö –¥–≤—É—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
            sample = [random.gauss(2, 0.5), random.gauss(2, 0.5), random.gauss(0, 0.5), random.gauss(0, 0.5)]
            label = 0
        elif random.random() < 0.5:
            # –ö–ª–∞—Å—Å 1: –±–æ–ª—å—à–∏–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø–µ—Ä–≤—ã—Ö –¥–≤—É—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
            sample = [random.gauss(-2, 0.5), random.gauss(-2, 0.5), random.gauss(0, 0.5), random.gauss(0, 0.5)]
            label = 1
        else:
            # –ö–ª–∞—Å—Å 2: –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–≤—É—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
            sample = [random.gauss(0, 0.5), random.gauss(0, 0.5), random.gauss(2, 0.5), random.gauss(2, 0.5)]
            label = 2
        
        X.append(sample)
        y.append(label)
    
    return X, y

def test_simple_network():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
    print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    X, y = create_simple_dataset()
    
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(X)} x {len(X[0])}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(set(y))}")
    
    print("\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
    network = SimpleNetwork()
    
    print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:")
    print("  Input: 4 features")
    print("  Hidden: 8 neurons + ReLU")
    print("  Output: 3 classes")
    
    print("\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    
    losses = []
    epochs = 20
    
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        
        for i in range(len(X)):
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            network.zero_grad()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –±–∞—Ç—á —Ä–∞–∑–º–µ—Ä–æ–º 1
            x_batch = [X[i]]
            y_batch = [y[i]]
            
            predictions = network.forward(x_batch)
            
            # –í—ã—á–∏—Å–ª—è–µ–º loss
            loss, softmax_pred = cross_entropy_loss(predictions, y_batch)
            total_loss += loss
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predicted_class = max(range(len(predictions[0])), key=lambda k: predictions[0][k])
            if predicted_class == y[i]:
                correct += 1
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            grad_loss = cross_entropy_backward(softmax_pred, y_batch)
            network.backward(grad_loss)
            network.update_weights()
        
        avg_loss = total_loss / len(X)
        accuracy = correct / len(X)
        losses.append(avg_loss)
        
        if epoch % 5 == 0 or epoch == epochs - 1:
            print(f"Epoch {epoch + 1:2d}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}")
    
    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–π —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"–ù–∞—á–∞–ª—å–Ω—ã–π loss: {losses[0]:.4f}")
    print(f"–ö–æ–Ω–µ—á–Ω—ã–π loss: {losses[-1]:.4f}")
    print(f"–°–Ω–∏–∂–µ–Ω–∏–µ loss: {losses[0] - losses[-1]:.4f}")
    
    is_decreasing = losses[0] > losses[-1]
    print(f"\nüéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏:")
    print(f"‚úì Loss –ø–∞–¥–∞–µ—Ç: {'–î–∞' if is_decreasing else '–ù–µ—Ç'}")
    print(f"‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞ {len(losses)} —ç–ø–æ—Ö: {'–î–∞' if len(losses) <= 20 else '–ù–µ—Ç'}")
    
    success = is_decreasing and len(losses) <= 20
    print(f"\n{'üéâ –£–°–ü–ï–•! –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è!' if success else '‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞.'}")
    
    # –ü—Ä–æ—Å—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è loss
    print(f"\nüìà –ì—Ä–∞—Ñ–∏–∫ loss (—Ç–µ–∫—Å—Ç–æ–≤—ã–π):")
    max_loss = max(losses)
    min_loss = min(losses)
    
    for i, loss in enumerate(losses):
        if i % 4 == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é 4-—é —ç–ø–æ—Ö—É
            normalized = int((loss - min_loss) / (max_loss - min_loss + 1e-8) * 50)
            bar = "‚ñà" * normalized + "‚ñë" * (50 - normalized)
            print(f"Epoch {i+1:2d}: {loss:.4f} |{bar}|")
    
    return success

if __name__ == "__main__":
    print("=" * 60)
    print("üéØ –ü–†–û–°–¢–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò")
    print("=" * 60)
    
    success = test_simple_network()
    
    print("\n" + "=" * 60)
    if success:
        print("‚úÖ –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
        print("üí° –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å–ø–æ—Å–æ–±–Ω–∞ –æ–±—É—á–∞—Ç—å—Å—è –∏ —Å–Ω–∏–∂–∞—Ç—å loss")
    else:
        print("‚ùå –¢–ï–°–¢ –ù–ï –ü–†–û–ô–î–ï–ù: –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞")
    print("=" * 60)