#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Optional, List

# –£—Å—Ç–∞–Ω–æ–≤–∏–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)

# ========== –ë–ê–ó–û–í–´–ï –ö–õ–ê–°–°–´ ==========

class Layer:
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
    """
    def __init__(self):
        self.training = True

    def forward(self, x):
        """
        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        """
        raise NotImplementedError

    def backward(self, grad_output):
        """
        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        """
        raise NotImplementedError

    def train(self):
        """
        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        """
        self.training = True

    def eval(self):
        """
        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        """
        self.training = False

    def __call__(self, x):
        return self.forward(x)


class ReLU(Layer):
    def __init__(self):
        super().__init__()
        self.input = None

    def forward(self, x):
        self.input = x
        output = np.maximum(0, x)
        return output

    def backward(self, grad_output):
        grad_input = grad_output.copy()
        grad_input[self.input <= 0] = 0
        return grad_input


class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
        self.output = None

    def forward(self, x):
        self.output = 1/(1+ np.exp(-x))
        return self.output

    def backward(self, grad_output):
        sigmoid_derivative = self.output * (1 - self.output)
        grad_input = sigmoid_derivative * grad_output
        return grad_input


class Tanh(Layer):
    def __init__(self):
        super().__init__()
        self.output = None

    def forward(self, x):
        self.output = np.tanh(x)
        self.output = np.clip(self.output, -0.999999, 0.999999)
        return self.output

    def backward(self, grad_output):
        tanh_derivative = 1 - self.output**2
        grad_input = tanh_derivative * grad_output
        return grad_input


class Linear(Layer):
    def __init__(self, input_size, output_size, bias=True):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.use_bias = bias

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ (Kaiming)
        std = np.sqrt(2.0 / input_size)
        self.weight = np.random.randn(input_size, output_size) * std

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è bias
        if self.use_bias:
            self.bias = np.zeros(output_size)
        else:
            self.bias = None

        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        self.input = None
        self.grad_weight = np.zeros_like(self.weight)
        if self.use_bias:
            self.grad_bias = np.zeros_like(self.bias)
        else:
            self.grad_bias = None

    def forward(self, x):
        self.input = x.copy()
        output = x @ self.weight

        if self.use_bias:
            output += self.bias

        return output

    def backward(self, grad_output):
        grad_input = grad_output @ self.weight.T
        self.grad_weight += self.input.T @ grad_output

        if self.use_bias:
            self.grad_bias += np.sum(grad_output, axis=0)

        return grad_input

    def update_weights(self, learning_rate=0.01):
        if self.grad_weight is not None:
            self.weight -= learning_rate * self.grad_weight

        if self.use_bias and self.grad_bias is not None:
            self.bias -= learning_rate * self.grad_bias

    def zero_grad(self):
        self.grad_weight = np.zeros_like(self.grad_weight)
        if self.use_bias:
            self.grad_bias = np.zeros_like(self.grad_bias)

    def parameters(self):
        if self.bias is not None:
            return (self.weight, self.bias)
        else:
            return (self.weight,)


class Sequential(Layer):
    def __init__(self, *layers):
        super().__init__()
        self.layers = list(layers)
        self.layer_outputs = []

    def add(self, layer):
        self.layers.append(layer)

    def forward(self, x):
        self.layer_outputs = []
        self.layer_outputs.append(x)
        output = x
        for layer in self.layers:
            output = layer.forward(output)
            self.layer_outputs.append(output)
        return output

    def backward(self, grad_output):
        grad = grad_output
        for layer in reversed(self.layers):
            grad = layer.backward(grad)
        return grad

    def update_weights(self, learning_rate=0.01):
        for layer in self.layers:
            if hasattr(layer, 'update_weights'):
                layer.update_weights(learning_rate)

    def zero_grad(self):
        for layer in self.layers:
            if hasattr(layer, 'zero_grad'):
                layer.zero_grad()

    def train(self):
        super().train()
        for layer in self.layers:
            layer.train()

    def eval(self):
        super().eval()
        for layer in self.layers:
            layer.eval()

    def __len__(self):
        return len(self.layers)

    def __getitem__(self, idx):
        return self.layers[idx]

    def parameters(self):
        for layer in self.layers:
            if hasattr(layer, 'parameters'):
                params = layer.parameters()
                if isinstance(params, tuple):
                    yield from params
                else:
                    yield params


class Dropout(Layer):
    def __init__(self, dropout_rate=0.5):
        super().__init__()
        self.dropout_rate = dropout_rate
        self.mask = None

    def forward(self, x):
        if self.training:
            self.mask = (np.random.rand(*x.shape) > self.dropout_rate).astype(np.float32) / (1.0 - self.dropout_rate)
            output = x * self.mask
        else:
            output = x
            self.mask = None
        return output

    def backward(self, grad_output):
        if self.training:
            grad_input = grad_output * self.mask
        else:
            grad_input = grad_output
        return grad_input


class BatchNorm(Layer):
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum

        # –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã gamma –∏ beta
        self.gamma = np.ones(num_features)
        self.beta = np.zeros(num_features)

        # –ù–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)

        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è backward pass
        self.batch_mean = None
        self.batch_var = None
        self.normalized = None
        self.input = None
        self.grad_gamma = np.zeros(num_features)
        self.grad_beta = np.zeros(num_features)

    def forward(self, x):
        self.input = x

        if self.training:
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–∫—É—â–µ–≥–æ batch
            self.batch_mean = np.mean(x, axis=0)
            self.batch_var = np.var(x, axis=0)

            # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–ò–°–ü–†–ê–í–õ–ï–ù–û)
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * self.batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * self.batch_var

            mean = self.batch_mean
            var = self.batch_var
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            mean = self.running_mean
            var = self.running_var

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        self.normalized = (x - mean) / np.sqrt(var + self.eps)

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–¥–≤–∏–≥
        output = self.gamma * self.normalized + self.beta

        return output

    def backward(self, grad_output):
        n = self.input.shape[0]
        # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        self.grad_gamma = np.sum(grad_output * self.normalized, axis=0)
        self.grad_beta = np.sum(grad_output, axis=0)

        grad_normalized = grad_output * self.gamma

        # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—Ö–æ–¥—É
        grad_var = np.sum(grad_normalized * (self.input - self.batch_mean), axis=0) * (-0.5) * (self.batch_var + self.eps)**(-1.5)

        grad_mean = np.sum(grad_normalized, axis=0) * (-1) / np.sqrt(self.batch_var + self.eps) + grad_var * np.mean(-2 * (self.input - self.batch_mean), axis=0)

        grad_input = (grad_normalized / np.sqrt(self.batch_var + self.eps)) + grad_var * 2 * (self.input - self.batch_mean) / n + grad_mean / n

        return grad_input

    def update_weights(self, learning_rate=0.01):
        if self.grad_gamma is not None:
            self.gamma -= learning_rate * self.grad_gamma

        if self.grad_beta is not None:
            self.beta -= learning_rate * self.grad_beta

    def zero_grad(self):
        self.grad_gamma = np.zeros(self.num_features)
        self.grad_beta = np.zeros(self.num_features)


# ========== –§–£–ù–ö–¶–ò–ò –ü–û–¢–ï–†–¨ ==========

def softmax(x):
    """–£—Å—Ç–æ–π—á–∏–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è softmax"""
    x_stable = x - np.max(x, axis=1, keepdims=True)
    exp_x = np.exp(x_stable)
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)


def one_hot_encode(labels, num_classes):
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ one-hot –∫–æ–¥–∏—Ä–æ–≤–∫—É"""
    batch_size = labels.shape[0]
    one_hot = np.zeros((batch_size, num_classes))
    one_hot[np.arange(batch_size), labels] = 1
    return one_hot


class CrossEntropyLoss:
    def __init__(self):
        self.predictions = None
        self.targets = None

    def forward(self, predictions, targets):
        self.predictions = predictions
        self.targets = targets

        # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º
        self.softmax_pred = softmax(predictions)

        # –í—ã—á–∏—Å–ª—è–µ–º cross-entropy loss
        batch_size = predictions.shape[0]
        correct_class_probs = self.softmax_pred[np.arange(batch_size), targets]
        loss = -np.mean(np.log(correct_class_probs + 1e-15))

        return loss

    def backward(self):
        batch_size = self.predictions.shape[0]
        num_classes = self.predictions.shape[1]

        # –°–æ–∑–¥–∞–µ–º one-hot –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è —Ü–µ–ª–µ–π
        one_hot_targets = one_hot_encode(self.targets, num_classes)

        # –ì—Ä–∞–¥–∏–µ–Ω—Ç: (softmax_pred - one_hot_targets) / batch_size
        grad = (self.softmax_pred - one_hot_targets) / batch_size
        return grad


# ========== –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† ==========

class AdamFixed:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps

        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è
        self.m = {}  # first moment
        self.v = {}  # second moment
        self._layer_steps = {}  # time steps for each layer

    def update(self, layer, layer_id):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ª–æ—è —Å –ø–æ–º–æ—â—å—é Adam"""
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —à–∞–≥–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ—è
        if layer_id not in self._layer_steps:
            self._layer_steps[layer_id] = 0
        
        self._layer_steps[layer_id] += 1
        t = self._layer_steps[layer_id]

        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        if hasattr(layer, 'grad_weight') and layer.grad_weight is not None:
            grad_w = layer.grad_weight
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–º–µ–Ω—Ç—ã –¥–ª—è –≤–µ—Å–æ–≤
            weight_key = f"{layer_id}_weight"
            if weight_key not in self.m:
                self.m[weight_key] = np.zeros_like(layer.weight)
                self.v[weight_key] = np.zeros_like(layer.weight)

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç (momentum)
            self.m[weight_key] = self.beta1 * self.m[weight_key] + (1 - self.beta1) * grad_w

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç (RMSprop)
            self.v[weight_key] = self.beta2 * self.v[weight_key] + (1 - self.beta2) * (grad_w ** 2)

            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è
            m_corrected = self.m[weight_key] / (1 - self.beta1 ** t)
            v_corrected = self.v[weight_key] / (1 - self.beta2 ** t)

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
            layer.weight -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.eps)

        # –û–±–Ω–æ–≤–ª—è–µ–º bias –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –≤–µ—Å–∞–º
        if hasattr(layer, 'grad_bias') and layer.grad_bias is not None:
            grad_b = layer.grad_bias
            
            bias_key = f"{layer_id}_bias"
            if bias_key not in self.m:
                self.m[bias_key] = np.zeros_like(layer.bias)
                self.v[bias_key] = np.zeros_like(layer.bias)

            self.m[bias_key] = self.beta1 * self.m[bias_key] + (1 - self.beta1) * grad_b
            self.v[bias_key] = self.beta2 * self.v[bias_key] + (1 - self.beta2) * (grad_b ** 2)

            m_corrected_b = self.m[bias_key] / (1 - self.beta1 ** t)
            v_corrected_b = self.v[bias_key] / (1 - self.beta2 ** t)

            layer.bias -= self.learning_rate * m_corrected_b / (np.sqrt(v_corrected_b) + self.eps)

        # –û–±–Ω–æ–≤–ª—è–µ–º gamma –∏ beta –¥–ª—è BatchNorm
        if hasattr(layer, 'grad_gamma') and layer.grad_gamma is not None:
            grad_gamma = layer.grad_gamma
            
            gamma_key = f"{layer_id}_gamma"
            if gamma_key not in self.m:
                self.m[gamma_key] = np.zeros_like(layer.gamma)
                self.v[gamma_key] = np.zeros_like(layer.gamma)

            self.m[gamma_key] = self.beta1 * self.m[gamma_key] + (1 - self.beta1) * grad_gamma
            self.v[gamma_key] = self.beta2 * self.v[gamma_key] + (1 - self.beta2) * (grad_gamma ** 2)

            m_corrected_gamma = self.m[gamma_key] / (1 - self.beta1 ** t)
            v_corrected_gamma = self.v[gamma_key] / (1 - self.beta2 ** t)

            layer.gamma -= self.learning_rate * m_corrected_gamma / (np.sqrt(v_corrected_gamma) + self.eps)

        if hasattr(layer, 'grad_beta') and layer.grad_beta is not None:
            grad_beta = layer.grad_beta
            
            beta_key = f"{layer_id}_beta"
            if beta_key not in self.m:
                self.m[beta_key] = np.zeros_like(layer.beta)
                self.v[beta_key] = np.zeros_like(layer.beta)

            self.m[beta_key] = self.beta1 * self.m[beta_key] + (1 - self.beta1) * grad_beta
            self.v[beta_key] = self.beta2 * self.v[beta_key] + (1 - self.beta2) * (grad_beta ** 2)

            m_corrected_beta = self.m[beta_key] / (1 - self.beta1 ** t)
            v_corrected_beta = self.v[beta_key] / (1 - self.beta2 ** t)

            layer.beta -= self.learning_rate * m_corrected_beta / (np.sqrt(v_corrected_beta) + self.eps)


# ========== –ó–ê–ì–†–£–ó–ß–ò–ö –î–ê–ù–ù–´–• ==========

class DataLoaderFixed:
    def __init__(self, X, y, batch_size=32, shuffle=False, drop_last=False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö

        Args:
            X: –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (features)
            y: –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (targets)
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            shuffle: –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
            drop_last: –æ—Ç–±—Ä–∞—Å—ã–≤–∞—Ç—å –ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –Ω–µ–ø–æ–ª–Ω—ã–π –±–∞—Ç—á
        """
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.drop_last = drop_last
        self.n_samples = len(X)
        
        self._reset()

    def _reset(self):
        """–°–±—Ä–æ—Å –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞"""
        self.indices = list(range(self.n_samples))
        if self.shuffle:
            np.random.shuffle(self.indices)
        self.current_idx = 0

    def __iter__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞"""
        self._reset()
        return self

    def __next__(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞"""
        if self.current_idx >= self.n_samples:
            raise StopIteration()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
        remaining = self.n_samples - self.current_idx
        current_batch_size = min(self.batch_size, remaining)
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á –Ω–µ–ø–æ–ª–Ω—ã–π –∏ drop_last=True, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
        if current_batch_size < self.batch_size and self.drop_last:
            raise StopIteration()

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
        batch_indices = self.indices[self.current_idx:self.current_idx + current_batch_size]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        batch_X = self.X[batch_indices]
        batch_y = self.y[batch_indices]
        
        self.current_idx += current_batch_size
        
        return batch_X.astype(np.float32), batch_y.astype(np.int32)

    def __len__(self):
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        if self.drop_last:
            return self.n_samples // self.batch_size
        else:
            return int(np.ceil(self.n_samples / self.batch_size))


# ========== –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ ==========

class NeuralNetworkFixed:
    def __init__(self, batch_size=128, epochs=10, learning_rate=0.001):
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
        self.model = Sequential(
            Linear(784, 512),
            BatchNorm(512),
            ReLU(),
            Dropout(0.4),
            
            Linear(512, 256),
            BatchNorm(256),
            ReLU(),
            Dropout(0.2),
            
            Linear(256, 128),
            BatchNorm(128),
            ReLU(),
            Dropout(0.2),
            
            Linear(128, 10)
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.optimizer = AdamFixed(learning_rate=learning_rate)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∫–∞–∫ —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞
        self.loss_func = CrossEntropyLoss()
        
        self.batch_size = batch_size
        self.epochs = epochs
        self.learning_rate = learning_rate

    def forward(self, x):
        return self.model.forward(x)

    def backward(self, grad_output):
        return self.model.backward(grad_output)

    def train_mode(self):
        self.model.train()

    def eval_mode(self):
        self.model.eval()

    def get_trainable_layers(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ–µ–≤ —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        trainable_layers = []
        for i, layer in enumerate(self.model.layers):
            if hasattr(layer, 'grad_weight') or hasattr(layer, 'grad_gamma'):
                trainable_layers.append((i, layer))
        return trainable_layers

    def compute_loss(self, predictions, targets):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å"""
        return self.loss_func.forward(predictions, targets)

    def compute_grad_loss(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å"""
        return self.loss_func.backward()

    def train_model(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        # –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        dataloader = DataLoaderFixed(X, y, batch_size=self.batch_size, shuffle=True, drop_last=True)
        
        # –ú–∞—Å—Å–∏–≤ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π loss
        losses = []
        
        self.train_mode()
        
        for epoch in range(self.epochs):
            epoch_losses = []
            print(f"Epoch {epoch + 1}/{self.epochs}")
            
            for batch_idx, (x_batch, y_batch) in enumerate(dataloader):
                # –û—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                self.zero_grad()
                
                # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                predictions = self.forward(x_batch)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
                loss = self.compute_loss(predictions, y_batch)
                epoch_losses.append(loss)
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                grad_loss = self.compute_grad_loss()
                self.backward(grad_loss)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
                self.update_weights()
                
                if batch_idx % 5 == 0:  # –ü–µ—á–∞—Ç–∞–µ–º –∫–∞–∂–¥—ã–µ 5 –±–∞—Ç—á–µ–π
                    print(f"  Batch {batch_idx}, Loss: {loss:.4f}")
            
            avg_loss = np.mean(epoch_losses)
            losses.append(avg_loss)
            print(f"  Average Loss: {avg_loss:.4f}")
            print()
        
        return losses

    def zero_grad(self):
        """–û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤—Å–µ—Ö —Å–ª–æ–µ–≤"""
        for layer in self.model.layers:
            if hasattr(layer, 'zero_grad'):
                layer.zero_grad()

    def update_weights(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤—Å–µ—Ö —Å–ª–æ–µ–≤"""
        trainable_layers = self.get_trainable_layers()
        for layer_id, layer in trainable_layers:
            self.optimizer.update(layer, layer_id)

    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö"""
        self.eval_mode()
        
        # –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        dataloader = DataLoaderFixed(X, np.zeros(len(X)), batch_size=self.batch_size, shuffle=False, drop_last=False)
        
        predictions = []
        for x_batch, _ in dataloader:
            result = self.forward(x_batch)
            batch_predictions = np.argmax(result, axis=1)
            predictions.extend(batch_predictions)
        
        return np.array(predictions)

    def evaluate_accuracy(self, X, y):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö"""
        predictions = self.predict(X)
        accuracy = np.mean(predictions == y)
        return accuracy


# ========== –°–û–ó–î–ê–ù–ò–ï –î–ê–ù–ù–´–• –ò –û–ë–£–ß–ï–ù–ò–ï ==========

def create_synthetic_dataset(num_samples=1000, input_dim=784, num_classes=10, seed=42):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
    np.random.seed(seed)
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    X = np.random.randn(num_samples, input_dim).astype(np.float32)
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏ —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –≤—Ö–æ–¥–∞–º–∏ –∏ –≤—ã—Ö–æ–¥–∞–º–∏
    weights = np.random.randn(input_dim, num_classes) * 0.01
    logits = X @ weights
    probabilities = softmax(logits)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    y = np.array([np.random.choice(num_classes, p=prob) for prob in probabilities])
    
    return X, y


def plot_training_curve(losses, title="Training Loss"):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ loss"""
    try:
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, len(losses) + 1), losses, 'b-', linewidth=2, label='Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(title)
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∏ –∫–æ–Ω–µ—á–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è loss
        plt.annotate(f'Start: {losses[0]:.4f}', 
                    xy=(1, losses[0]), xytext=(len(losses)*0.2, losses[0]*1.1),
                    arrowprops=dict(arrowstyle='->', color='red'),
                    fontsize=10, color='red')
        
        plt.annotate(f'End: {losses[-1]:.4f}', 
                    xy=(len(losses), losses[-1]), xytext=(len(losses)*0.8, losses[-1]*1.1),
                    arrowprops=dict(arrowstyle='->', color='green'),
                    fontsize=10, color='green')
        
        plt.tight_layout()
        plt.savefig('training_loss.png', dpi=150, bbox_inches='tight')
        print("üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ 'training_loss.png'")
        plt.show()
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫: {e}")
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"–ù–∞—á–∞–ª—å–Ω—ã–π loss: {losses[0]:.4f}")
    print(f"–ö–æ–Ω–µ—á–Ω—ã–π loss: {losses[-1]:.4f}")
    print(f"–°–Ω–∏–∂–µ–Ω–∏–µ loss: {losses[0] - losses[-1]:.4f} ({((losses[0] - losses[-1])/losses[0]*100):.1f}%)")
    
    return losses[0] > losses[-1]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º True –µ—Å–ª–∏ loss –ø–∞–¥–∞–µ—Ç


def train_and_evaluate_network():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è, –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
    print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    X, y = create_synthetic_dataset(num_samples=1000, input_dim=784, num_classes=10, seed=42)
    
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {X.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(np.unique(y))}")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y)}")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")
    
    print("\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å
    network = NeuralNetworkFixed(
        batch_size=64,
        epochs=15,
        learning_rate=0.001
    )
    
    print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏:")
    for i, layer in enumerate(network.model.layers):
        print(f"  {i}: {layer.__class__.__name__}")
    
    print("\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    
    # –û–±—É—á–∞–µ–º —Å–µ—Ç—å
    losses = network.train_model(X_train, y_train)
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ loss
    print("\nüìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ loss...")
    is_decreasing = plot_training_curve(losses, "Training Loss - Synthetic Dataset")
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüîç –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_accuracy = network.evaluate_accuracy(X_test, y_test)
    train_accuracy = network.evaluate_accuracy(X_train, y_train)
    
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–π —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
    print("\nüéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏:")
    print(f"‚úì Loss –ø–∞–¥–∞–µ—Ç: {'–î–∞' if is_decreasing else '–ù–µ—Ç'}")
    print(f"‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞ {len(losses)} —ç–ø–æ—Ö: {'–î–∞' if len(losses) <= 20 else '–ù–µ—Ç'}")
    
    success = is_decreasing and len(losses) <= 20
    print(f"\n{'üéâ –£–°–ü–ï–•! –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è!' if success else '‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞.'}")
    
    return network, losses, train_accuracy, test_accuracy


if __name__ == "__main__":
    print("=" * 60)
    print("üéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò")
    print("=" * 60)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    network, losses, train_acc, test_acc = train_and_evaluate_network()